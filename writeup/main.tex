\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, siunitx, physics}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{caption}

\geometry{margin=1in}

\title{\textbf{4 $\times$ 4 CMOS Array Multiplier} \\}
       \vspace{0.5cm}

\author{Krishna Karthikeya Chemudupati \\ Adithya Selvakumar \\}
\date{November 13, 2025}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage

\tableofcontents

\newpage

% ----------------------------------------------------
\section{Introduction and Background}



\newpage

% ----------------------------------------------------
\section{Baseline Design}



\newpage

% ----------------------------------------------------
\section{Baseline Functional Verification}
\label{sec:functional_verification}

To verify the correctness of both the baseline and optimized 4$\times$4 multiplier implementations, we built a fully exhaustive, self-checking testbench around the transistor-level schematic. The testbench (1) applies all $2^8 = 256$ possible input combinations to the multiplier, (2) samples the eight output bits only after they have settled, and (3) automatically checks that every sampled output word matches the ideal product $Z = X \cdot Y$.

\subsection{Exhaustive Transient Stimulus}

The testbench consists of the multiplier under test driven by eight independent \texttt{vpulse} sources, one per primary input bit ($X_0$–$X_3$ and $Y_0$–$Y_3$). Each source is configured with a different period so that, taken together, the inputs realize a hardware binary counter:

\begin{center}
\begin{tabular}{c c}
$X_0$: period $T = 10\,\mathrm{ns}$, & $Y_0$: period $16T$ \\
$X_1$: period $2T$,                  & $Y_1$: period $32T$ \\
$X_2$: period $4T$,                  & $Y_2$: period $64T$ \\
$X_3$: period $8T$,                  & $Y_3$: period $128T$
\end{tabular}
\end{center}

With a 50\% duty cycle on each bit, this configuration causes the eight inputs to count through all binary patterns from 0 to 255. Over a total simulation time of $1.28\,\mu\mathrm{s} = 128T$, the testbench therefore applies every possible unsigned input pair $(X,Y)$ with $X,Y \in [0,15]$. The corresponding Cadence waveform capture is shown in Fig.~\ref{fig:baseline_signals}, where the lower traces are the input bits and the upper traces are the multiplier outputs $Z_0$–$Z_7$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/baseline_verification_signals.png}
  \caption{Transient response of the 4$\times$4 multiplier under exhaustive binary-count stimulus. The eight input bits form a hardware counter, and the eight output bits respond to each new input vector.}
  \label{fig:baseline_signals}
\end{figure}

Because this verification runs on the full transistor-level schematic, it naturally captures the effects of propagation delay, glitching, and finite output slew; no behavioral or idealized models are used.

\subsection{Sampling and Logic-Level Conversion}

The simulator output is exported to CSV and processed in Python. Rather than sampling at arbitrary times, we choose one time point in the middle of each interval where the inputs are guaranteed to be static. The shortest time between two transitions on any input is $T/2$ (the high or low half of $X_0$'s period), so we define the $k$th sampling instant as
\[
t_k = \frac{kT}{2} + \frac{T}{4}, \qquad k = 0,1,\dots,255.
\]
At $t_k$ the counter has been stable for at least $T/4$ and the multiplier output has had time to settle to its final value.

For each $t_k$, the script locates the nearest time point in the exported CSV and extracts the voltages for all input and output nodes. These analog voltages are then converted to logic levels using conservative thresholds:
\[
V > 1.0\,\mathrm{V} \Rightarrow \text{logic 1}, \qquad
V < 0.2\,\mathrm{V} \Rightarrow \text{logic 0}.
\]
Values that fall between 0.2\,V and 1.0\,V are marked as ``unsettled'' (stored as NaN) instead of being forced to a digital 0 or 1. This makes the check robust to slow transitions or ringing: if any output bit has not reached a valid logic level by the sampling time, the issue is surfaced immediately in the processed data rather than silently misclassified.

The result of this first Python stage is a new CSV file with exactly 256 rows (one per input vector) and clean digital values for all bits $X_3$–$X_0$, $Y_3$–$Y_0$, and $Z_7$–$Z_0$.

\subsection{Reconstruction of Integer Operands and Products}

A second Python script interprets the bit fields as integers and checks the arithmetic. A practical concern when exporting from Cadence is that the bit order in the CSV may not match the expected LSB-to-MSB ordering. To guard against this, the script first analyzes the number of transitions on each bit; the bit with the highest toggle count must be the LSB (it has the shortest period). If the fastest-toggling column is labeled $X_3$ instead of $X_0$, for example, the script concludes that the bits are reversed and flips the order in software before proceeding. The same logic is applied to the $Y$ inputs. This automatic detection makes the verification insensitive to column-ordering mistakes in the export step.

With the bit ordering corrected, the integer values are reconstructed as
\begin{align*}
A &= X_0 + 2X_1 + 4X_2 + 8X_3, \\
B &= Y_0 + 2Y_1 + 4Y_2 + 8Y_3, \\
Z &= Z_0 + 2Z_1 + 4Z_2 + 8Z_3 + 16Z_4 + 32Z_5 + 64Z_6 + 128Z_7.
\end{align*}
These become the columns \texttt{A\_val}, \texttt{B\_val}, and \texttt{Z\_val}. The correct reference result is simply
\[
\texttt{Expected} = \texttt{A\_val} \times \texttt{B\_val}.
\]
For each of the 256 vectors the script sets a Boolean flag \texttt{Pass} if \texttt{Z\_val == Expected}. A short summary printed at the end of the run reports how many of the 256 cases passed and, if any failed, lists the mismatched vectors with their input pair, measured product, and expected product. For both the baseline and optimized multipliers, the script reports 256/256 passing cases.

\subsection{Verification Table Visualization}

To make the results easy to interpret at a glance, a final Python script converts the verified dataset into the 16$\times$16 table shown in Fig.~\ref{fig:baseline_verification_table}. The horizontal axis corresponds to the integer value of $X$, the vertical axis corresponds to $Y$, and each cell encodes the product for one $(X,Y)$ pair:

\begin{itemize}
  \item The upper-left number in each cell is the measured product $Z$ from the simulation.
  \item The lower-right number is the ideal product $X \cdot Y$.
  \item The cell background is colored green if these two numbers agree and red otherwise.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/baseline_verification_visualization.png}
  \caption{Visualization of functional verification for the 4$\times$4 multiplier. For each input pair $(X,Y)$, the upper-right number is the simulated product and the lower-left number is the ideal product $X \cdot Y$. Matching cells are shaded green; any mismatch would appear as a red cell.}
  \label{fig:baseline_verification_table}
\end{figure}

Because the multiplier is correct, every cell in Fig.~\ref{fig:baseline_verification_table} is green and the two numbers in each cell are identical. Any systematic wiring error (for example, swapping two output bits or mis-interpreting signedness) would create a visible pattern of red cells, making this visualization a powerful sanity check in addition to the numerical pass/fail summary. The same scripts are reused for both the baseline and optimized designs, so any changes to the circuit can be re-verified quickly and objectively using identical criteria.

\newpage

% ----------------------------------------------------
\section{Baseline Delay Measurement}

\subsection{Worst–Case Delay Characterization}

To determine the worst–case delay of the $4 \times 4$ array multiplier, we examine the
structure of the adder array and identify the longest carry–dependent computation path.
Each output bit $Z_k$ is formed by a column of full adders (FAs) and half adders (HAs)
whose inputs depend on the partial products $X_i Y_j$ and on the carry values
generated by preceding adders. A delay ``bottleneck'' occurs whenever an adder must
wait for its carry--in to settle before producing either its sum or carry--out. Thus, the
critical path corresponds to the path along which the largest number of carry--dependent
operations occur sequentially.

\subsubsection{Critical Path Identification}

Among all output bits, $Z_7$ is the furthest from the first carry--producing element and
therefore admits the longest possible chain of dependent adders. The first carry is
introduced at the $Z_1$ half adder, and from this point, the carry must propagate
diagonally across the array through a sequence of full adders before eventually reaching
the most significant output $Z_7$. Although multiple geometric paths exist from
$Z_1$ to $Z_7$, each path contains the same total number of carry--in to carry--out
transitions and the same number of carry--in to sum transitions. Because the array
multiplier is topologically symmetric, all such paths have identical logical depth.

For a $4 \times 4$ array, the critical path contains:
\begin{itemize}
    \item six FA/HAs contributing a $\text{Cin} \rightarrow \text{Cout}$ delay (horizontal movement), and
    \item two FA/HAs contributing a $\text{Cin} \rightarrow \text{Sum}$ delay (vertical movement).
\end{itemize}
One representative critical path is:
\[
\text{AND} 
\rightarrow \text{HA}_{Z_1}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}_{Z_7},
\]
where the ordering reflects successive carry propagation through the array.

\subsubsection{Adder Operating States and Worst--Case Delay Conditions}

An adder (FA or HA) can operate in one of three logical states depending on the values
of its two data inputs $(A, B)$:
\begin{enumerate}
    \item \textbf{Terminate (T):} $(A,B) = (0,0)$.
    In this state, the sum equals $\text{Cin}$, but the carry--out is always $0$.  
    This state prematurely stops the carry chain and therefore does \emph{not} contribute
    to the worst--case delay.
    \item \textbf{Propagate (P):} $(A,B) \in \{(0,1), (1,0)\}$.
    Both sum and carry--out depend on $\text{Cin}$, allowing the incoming carry to ripple
    through the adder.  
    This is the state required to maximize delay.
    \item \textbf{Generate (G):} $(A,B) = (1,1)$.
    The carry--out is forced to logic high regardless of $\text{Cin}$, immediately terminating
    the dependency on previous stages.  
    This state shortens the delay and therefore must be avoided on the critical path.
\end{enumerate}

To excite the worst--case delay, \emph{every adder along the identified critical path must
be placed in the propagate state}. Once all adders are in propagate mode, toggling the
carry--in of the first HA will cause a carry transition to ripple uninterrupted across the
entire chain, from $Z_1$ through to $Z_7$.

\subsubsection{Worst--Case Input Vector Selection}

Let $X_0$ be the least significant bit of $X$ and $Y_0$ the least significant bit of $Y$.
To create a clean rising transition that initiates the carry propagation, $X_0$ is toggled
from $0$ to $1$ while ensuring that the multiplier inputs enforce the propagate state
at all adders along the critical path. One valid worst--case input assignment is:

\[
\begin{array}{c|c|l}
\text{Input Bit} & \text{Value} & \text{Effect} \\ \hline
X_0 & 0 \rightarrow 1 & \text{Introduces a rising carry stimulus at HA}_{Z_1} \\
X_1 & 0 & \text{Ensures propagate state at next HA} \\
X_2 & 0 & \text{Propagate state for deeper FAs} \\
X_3 & 1 & \text{Propagate state in upper FA chain} \\ \hline
Y_0 & 1 & \text{Ensures HA input is capable of seeing Cin transition} \\
Y_1 & 1 & \text{Same as above} \\
Y_2 & 1 & \text{Maintains propagate condition in intermediate FAs} \\
Y_3 & 1 & \text{Maintains propagate condition at upper FAs} \\
\end{array}
\]

With these inputs, all adders along the $Z_1 \rightarrow Z_7$ critical path reside in the
propagate state, ensuring that a single transition at $\text{Cin}$ traverses the full logical
depth of the array. Measuring the time difference between the transition at $X_0$ and
the corresponding transition observed at $Z_7$ yields the worst--case propagation delay
of the baseline multiplier.

\newpage

\subsection{Worst-Case Delay Test Schematic}



\newpage

\subsection{Worst-Case Delay Analysis}

\subsubsection{Worst Case Propagation for Switching Input 0 $\rightarrow$ 1}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{writeup//figures/baseline_delay_LH_signals.png}
    \caption{Enter Caption}
    \label{fig:baseline_delay_LH_signals}
\end{figure}

To extract the worst--case delay, the multiplier was excited with the input pattern
described in Section 4.1.3, ensuring that all adders along the
$Z_1 \rightarrow Z_7$ diagonal operate in the propagate state. A rising transition was
applied to $X_0$, which produces the first carry at the $Z_1$ half adder and initiates a
carry ripple through all adders on the critical path.

The propagation delay $t_{\mathrm{PLH}}$ was measured using the $50\%$ voltage points of
the input and output waveforms. The $X_0$ input crosses $0.6\,$V at
\[
t_{\text{in,50\%}} = 10.017844\ \text{ns},
\]
and the most significant output bit $Z_7$ crosses $0.6\,$V at
\[
t_{\text{out,50\%}} = 10.214481\ \text{ns}.
\]

The low--to--high propagation delay is therefore
\[
t_{\mathrm{PLH}}
= t_{\text{out,50\%}} - t_{\text{in,50\%}}
= 10.214481\ \text{ns} - 10.017844\ \text{ns}
= 0.196637\ \text{ns}.
\]

Thus, the worst--case delay of the baseline multiplier is
\[
\boxed{t_{\mathrm{PLH}} = 196.637\ \text{ps}}
\]

\subsubsection{Worst Case Propagation for Switching Input 1 $\rightarrow$ 0}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{writeup//figures/baeline_delay_HL_signals.png}
    \caption{Enter Caption}
    \label{fig:baseline_delay_LH_signals}
\end{figure}

The high--to--low worst--case delay was obtained using the same propagate--state
input vector used for the low--to--high measurement. In this case, $X_0$ transitions
from logic high to logic low, and the resulting carry discharge travels along the
$Z_1 \rightarrow Z_7$ critical path.

The $50\%$ crossing of the input $X_0$ waveform occurs at
\[
t_{\text{in,50\%}} = 15.014775\ \text{ns},
\]
while the $50\%$ crossing of the output $Z_7$ waveform occurs at
\[
t_{\text{out,50\%}} = 15.208198\ \text{ns}.
\]

Thus, the high--to--low propagation delay is
\[
t_{\mathrm{PHL}}
= t_{\text{out,50\%}} - t_{\text{in,50\%}}
= 15.208198\ \text{ns} - 15.014775\ \text{ns}
= 0.193423\ \text{ns}.
\]

Therefore, the worst--case $t_{\mathrm{PHL}}$ of the baseline multiplier is
\[
\boxed{t_{\mathrm{PHL}} = 193.423\ \text{ps}}
\]

\newpage

% ----------------------------------------------------
\section{Baseline Active Energy Measurement}



Max switching

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_active_energy_max_signals.png}
    \caption{Enter Caption}
    \label{fig:baseline_active_energy_max_signals}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_active_energy_max_value.png}
    \caption{Enter Caption}
    \label{fig:baseline_active_energy_max_value}
\end{figure}

\newpage

Avg Switching



\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_active_energy_avg_value.png}
    \caption{Enter Caption}
    \label{fig:baseline_active_energy_avg_value}
\end{figure}

\newpage

% ----------------------------------------------------
\section{Baseline Leakage Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Design}

\subsection{Worst Case Delay}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\linewidth]{writeup/figures/arraymult_criticalpath.png}
  \caption{Critical-path staircases in a 4×4 Braun array multiplier. X$_0$–X$_3$ are the N bits (multiplicand) and Y$_0$–Y$_3$ are the M bits (multiplier). Figure adapted from materials by Prof. Janakiraman Viraraghavan, IIT Madras.}
\end{figure}

\noindent
We analyze the 4×4 Braun (array) multiplier shown above. The multiplicand bits are X$_0$–X$_3$ (the N bits) and the multiplier bits are Y$_0$–Y$_3$ (the M bits). Each partial product x$_i$y$_j$ is generated by a static-CMOS AND gate, and these partial products are reduced by a regular grid of half adders (HAs) and full adders (FAs). Within a column, carries propagate downward; between columns, sums propagate leftward. The slow paths are the northeast-to-southwest staircases that begin at a right-edge partial product and alternate vertical carry hops with horizontal sum hops until they reach the lower-left outputs.

\paragraph{Delay primitives and convention}
We define three cell delays under the loading and drive conditions that match the tiled array:
\[
t_{\text{and}} \text{ for the partial-product AND,}\quad
t_{\text{carry}} \text{ for an HA/FA carry-out,}\quad
t_{\text{sum}} \text{ for an HA/FA sum-out.}
\]
In static-CMOS adders the sum network is built from XOR/XNOR structures that have larger logical effort and parasitic capacitance than the carry (majority / AND-OR) network, so typically
\[
t_{\text{sum}} > t_{\text{carry}}.
\]

\paragraph{Exciting the limiting staircase}
The worst transition is not the input vector that flips the most bits. It is the transition that (i) toggles a right-edge partial product and (ii) places each encountered adder in propagate mode (a$\oplus$b = 1 and ab = 0), forcing a newly injected carry to ripple through all stages along a staircase to the lower-left.

\paragraph{Counting hops on an M×N array}
For any such staircase in an M×N array, the stage counts are:
\[
\begin{aligned}
\text{carry hops}   &= (M-1) + (N-2),\\
\text{sum hops}     &= (M-1),
\end{aligned}
\]
plus the initial $t_{\text{and}}$ that launches the path. Therefore the generic array-multiplier timing is
\[
t_{\text{array}}(M,N) \;=\; \big[(M-1) + (N-2)\big]\,t_{\text{carry}}
\;+\; (M-1)\,t_{\text{sum}}
\;+\; t_{\text{and}}.
\]

\paragraph{Specialization to the 4×4 case}
With $M=N=4$ we obtain five carry hops and three sum hops:
\[
t_{\text{path}} \;=\; 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}.
\]
In the 4×4 topology the bottom-left FA drives two neighboring outputs: its carry is Z$_7$ and its sum is Z$_6$. The staircase counted above terminates at the sum of this FA, so
\[
\boxed{\,t_{Z_6} = 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}\,}.
\]
Because $t_{\text{sum}} > t_{\text{carry}}$ for static-CMOS FAs, Z$_6$ is slower than the otherwise identical staircase that would end at the carry output Z$_7$. Hence the worst-case delay of the 4×4 array ends at Z$_6$, not Z$_7$.

\paragraph{Implications}
Multiple staircases (such as CP$_1$ and CP$_2$ in the figure) have the same hop counts and are nearly iso-delay, so cell sizing must balance stage effort across the grid. Nevertheless, the decisive last stage is the sum network of the bottom-left FA that produces Z$_6$, and this stage should be weighted accordingly when sizing to minimize the overall worst-case delay.

\subsection{Schematics in Static CMOS}
\subsubsection{Full Adder (FA)}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{writeup/figures/staticcmos_fa_rabaey_annotated.png}
  \caption{Complementary static-CMOS full adder. Figure adapted from Rabaey.}
\end{figure}

\noindent
The full adder computes the carry-out $C_o$ and sum $S$ from inputs $A$, $B$, and $C_i$.  
We employ a complementary static-CMOS realization that shares intermediate logic between
the carry and sum paths. The logic is organized as
\[
C_o = AB + BC_i + AC_i \quad\text{(3-input majority)}, \qquad
S = ABC_i + C_o\,(A + B + C_i).
\]
The schematic implements these expressions with series–parallel $p$- and $n$-networks so that
each output is produced by static pull-up and pull-down paths, guaranteeing full-swing,
monotonic transitions and large noise margins.

\paragraph{Signal flow and stage counts}
The circuit is partitioned around an internal node $X$ that realizes the reorganized carry logic.
\begin{itemize}
  \item \textbf{Path to $C_o$ (two static stages).} A complex CMOS network first forms $X$,
  which captures the majority function. An output inverter then buffers $X$ to produce $C_o$.
  \item \textbf{Path to $S$ (three static stages).} The same $X$ is reused by a second
  static network that combines $X$ with $A$, $B$, and $C_i$ to generate a pre-sum node.
  A final inverter buffers this node to produce $S$.
\end{itemize}
Thus, $C_o$ emerges after two static stages and $S$ after three. While not ``single-stage''
at the pins, these are compact paths with low internal fanout and limited parasitic loading.

\paragraph{Why static CMOS over a bag of gates}
A gate-assembled FA (e.g., library XOR/AND/OR composition) typically yields a deeper chain:
$C_o$ commonly requires two XOR-class stages, and $S$ often traverses a sequence such as
XOR $\rightarrow$ NAND $\rightarrow$ NOT $\rightarrow$ NOR $\rightarrow$ NOT.
Each additional logic boundary adds logical effort and parasitic capacitance, inflating
end-to-end delay and degrading slews. The static-CMOS realization reduces the number of
boundaries, shares $X$ rather than duplicating heavy networks, and eliminates large internal
fanouts between discrete gates. Because both outputs are produced by complementary static
networks and then buffered, signals remain rail-to-rail and monotonic across many cascaded cells.

\paragraph{Fit for the array multiplier}
In the 4$\times$4 Braun array, the slow paths are the northeast-to-southwest staircases of adders,
and the measured worst case terminates at the sum output of the bottom-left FA ($Z_6$).
Replacing multi-gate chains with the compact static implementation shortens the effective
stage depth on these staircases and reduces internal parasitics, lowering the overall delay.

\noindent At the same time, static-CMOS robustness (no ratioing, no dynamic storage) ensures predictable
behavior when many FA cells are tiled: outputs do not droop through the grid, hazards are
suppressed by the static topologies and output inverters, and uniform polarity avoids the need
for multiple FA variants or extra inverters during integration.

\noindent
In summary, the Rabaey complementary static-CMOS FA provides a favorable speed–robustness tradeoff
for the multiplier: fewer and better stages than a bag-of-gates implementation, shared intermediate
logic that limits internal loading, and full-swing stability that holds when cascading many cells.

\subsubsection{Half Adder (HA)}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_xor.png}\hfill
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_nand.png}
  \caption{Static-CMOS XOR (left) for Sum and NAND (right) used with a following inverter for Carry. Figure adapted from Shakshat Virtual Lab, IIT Guwahati}
\end{figure}

\noindent
The half adder takes one-bit inputs $A$ and $B$ and produces outputs \textit{Sum} and \textit{Carry}. From the truth table,
\[
\text{Sum} = A \oplus B,
\qquad
\text{Carry} = A \cdot B = \overline{\mathrm{NAND}(A,B)}.
\]

\noindent
Implementation in complementary static CMOS is therefore direct:
\begin{itemize}
  \item \textbf{Sum path.} Realize $A \oplus B$ with a static-CMOS XOR gate. This is a single logic stage at the Sum pin with full-swing, monotonic behavior.
  \item \textbf{Carry path.} Realize $A \cdot B$ as a static-CMOS NAND followed by a static inverter: $\text{Carry}=\overline{\mathrm{NAND}(A,B)}$. This is two logic stages at the Carry pin (NAND then inverter), also full swing and monotonic.
\end{itemize}

\noindent
Unlike the full adder, there is no third input to exploit for algebraic sharing or logic reorganization; the minimal Boolean forms are already $A \oplus B$ and $A \cdot B$. Consequently, the “optimized” static-CMOS HA coincides with the baseline bag-of-gates realization: one XOR stage for Sum and a NAND+inverter pair for Carry. This keeps logic depth minimal for each output while retaining the robustness advantages of static CMOS (rail-to-rail levels, large noise margins, and predictable cascading behavior in the multiplier array).

\subsection{Optimizing Gate Sizes Along CP1}

\paragraph{Design variables and device mapping.}
We size five gate types with continuous, positive scale factors
\[
k_{\text{nand}},\;k_{\text{inv}},\;k_{\text{fa1}},\;k_{\text{fa2}},\;k_{\text{xor}}>0.
\]
For the \textbf{NAND} gate, both PMOS and NMOS widths equal $k_{\text{nand}}$.
For the \textbf{INV}, \textbf{FA Stage~1}, \textbf{FA Stage~2}, and \textbf{XOR}, the PMOS width is $2k$ and the NMOS width is $k$, where $k$ is the corresponding scale ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$, $k_{\text{xor}}$).

\paragraph{Assumptions and units.}
We assume: (i) diffusion capacitances are neglected ($C_d=0$);
(ii) all inputs are driven by a minimum-size inverter; (iii) pull-up resistance $R_{\!up}=2R_{\!un}$;
(iv) the final output load is $10C_g$. We measure delay in units of $R_{\!un}C_g$; i.e., we factor out the common multiplier $R_{\!un}C_g$ from every $R_i C_j$ product.

\paragraph{Definition of CP1.}
The critical path instance CP1 crosses the following gate sequence:
\[
\begin{aligned}
&\text{AND }(\text{NAND}+\text{INV});\ 
\text{HA,carry }(\text{NAND}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\\
&\text{HA,sum }(\text{INV}+\text{XOR});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV});\\
&\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV}).
\end{aligned}
\]

\paragraph{Step-by-step RC model (21 steps).}
Let $(R_i, C_i)$ denote the driving resistance and the load capacitance of step $i$ on CP1, each expressed in the normalized units described above. Using the worst-case edge per step you specified:

\[
\begin{aligned}
R_1&=2,                      & C_1&=2k_{\text{nand}}, \\
R_2&=\frac{2}{k_{\text{nand}}}, & C_2&=3k_{\text{inv}}, \\
R_3&=\frac{2}{k_{\text{inv}}},   & C_3&=2k_{\text{nand}}, \\
R_4&=\frac{2}{k_{\text{nand}}}, & C_4&=3k_{\text{inv}}, \\
R_5&=\frac{2}{k_{\text{inv}}},   & C_5&=6k_{\text{fa1}}, \\
R_6&=\frac{6}{k_{\text{fa1}}},   & C_6&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_7&=\frac{2}{k_{\text{inv}}},   & C_7&=6k_{\text{fa1}}, \\
R_8&=\frac{6}{k_{\text{fa1}}},   & C_8&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_9&=\frac{2}{k_{\text{inv}}},   & C_9&=3k_{\text{inv}}, \\
R_{10}&=\frac{2}{k_{\text{inv}}},& C_{10}&=3k_{\text{xor}}, \\
R_{11}&=\frac{4}{k_{\text{xor}}},& C_{11}&=6k_{\text{fa1}}, \\
R_{12}&=\frac{6}{k_{\text{fa1}}},& C_{12}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{13}&=\frac{2}{k_{\text{inv}}},& C_{13}&=6k_{\text{fa1}}, \\
R_{14}&=\frac{6}{k_{\text{fa1}}},& C_{14}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{15}&=\frac{4}{k_{\text{fa2}}},& C_{15}&=3k_{\text{inv}}, \\
R_{16}&=\frac{2}{k_{\text{inv}}},& C_{16}&=6k_{\text{fa1}}, \\
R_{17}&=\frac{6}{k_{\text{fa1}}},& C_{17}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{18}&=\frac{2}{k_{\text{inv}}},& C_{18}&=6k_{\text{fa1}}, \\
R_{19}&=\frac{6}{k_{\text{fa1}}},& C_{19}&=3k_{\text{inv}}+3k_{\text{fa2}}, \\
R_{20}&=\frac{4}{k_{\text{fa2}}},& C_{20}&=3k_{\text{inv}}, \\
R_{21}&=\frac{2}{k_{\text{inv}}},& C_{21}&=10. \\
\end{aligned}
\]

\paragraph{Elmore delay objective.}
Let the cumulative resistance up to step $i$ be
\[
S_i \;\triangleq\; \sum_{j=1}^{i} R_j.
\]
The Elmore delay along CP1 is
\[
\tau(k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}})
= \sum_{i=1}^{21} S_i\,C_i.
\]
By construction, $\tau$ is homogeneous in the time unit $R_{\!un}C_g$.

\paragraph{Structure of the derivatives.}
Each $R_i$ is either constant or of the form $\alpha_i/x$ for a single sizing variable $x\in
\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}$; each $C_i$ is either constant or of the form $\beta_i x$ or a sum of such linear terms. Hence
\[
\frac{\partial R_i}{\partial x}=
\begin{cases}
-\dfrac{\alpha_i}{x^2}, & \text{if } R_i=\dfrac{\alpha_i}{x},\\[6pt]
0, & \text{otherwise},
\end{cases}
\qquad
\frac{\partial C_i}{\partial x}=
\begin{cases}
\beta_i, & \text{if } C_i\text{ contains }\beta_i x,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Using $S_i=\sum_{j\le i}R_j$, its derivative satisfies
\[
\frac{\partial S_i}{\partial x}=\sum_{j=1}^{i}\frac{\partial R_j}{\partial x}.
\]
Therefore the gradient components are
\[
\boxed{\quad
\frac{\partial \tau}{\partial x}
= \sum_{i=1}^{21}\left(\frac{\partial S_i}{\partial x}\,C_i
+ S_i\,\frac{\partial C_i}{\partial x}\right)
= \sum_{i=1}^{21}\left(\sum_{j=1}^i \frac{\partial R_j}{\partial x}\right)C_i
+ \sum_{i=1}^{21} S_i\,\frac{\partial C_i}{\partial x}.
\quad}
\]

\paragraph{Coefficient bookkeeping per variable.}
For compactness, we list the steps that contribute to each variable’s partial derivative, with their coefficients $(\alpha,\beta)$:

\begin{itemize}
\item \textbf{$x=k_{\text{nand}}$:}
\[
\begin{aligned}
&\text{Resistance terms: } R_2=\tfrac{2}{k_{\text{nand}}},\ R_4=\tfrac{2}{k_{\text{nand}}}
\ \Rightarrow\ \alpha_{2}=\alpha_{4}=2;\\
&\text{Capacitance terms: } C_1=2k_{\text{nand}},\ C_3=2k_{\text{nand}}
\ \Rightarrow\ \beta_{1}=\beta_{3}=2.
\end{aligned}
\]

\item \textbf{$x=k_{\text{inv}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_3,R_5,R_7,R_9,R_{10},R_{13},R_{16},R_{18},R_{21}
= \tfrac{2}{k_{\text{inv}}}\ \Rightarrow\ \alpha=2\ \text{at those indices};\\
&\text{Capacitance: } C_2,C_4,C_6,C_8,C_9,C_{12},C_{14},C_{15},C_{17},C_{19},C_{20}
= 3k_{\text{inv}}\ \Rightarrow\ \beta=3\ \text{at those indices}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa1}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_6,R_8,R_{12},R_{14},R_{17},R_{19}=\tfrac{6}{k_{\text{fa1}}}
\ \Rightarrow\ \alpha=6\ \text{at those indices};\\
&\text{Capacitance: } C_5,C_7,C_{11},C_{13},C_{16},C_{18}=6k_{\text{fa1}},\\
&\hspace*{25mm}
C_6,C_8,C_{12},C_{14},C_{17},C_{19}\ \text{contain }3k_{\text{fa1}}
\ \Rightarrow\ \beta=6\ \text{or }3\ \text{accordingly}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa2}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_{15},R_{20}=\tfrac{4}{k_{\text{fa2}}}
\ \Rightarrow\ \alpha=4\ \text{at }15,20;\\
&\text{Capacitance: } C_{19}=3k_{\text{fa2}}
\ \Rightarrow\ \beta_{19}=3.
\end{aligned}
\]

\item \textbf{$x=k_{\text{xor}}$:}
\[
\text{Resistance: } R_{11}=\tfrac{4}{k_{\text{xor}}}\ (\alpha_{11}=4),\qquad
\text{Capacitance: } C_{10}=3k_{\text{xor}}\ (\beta_{10}=3).
\]
\end{itemize}

\paragraph{Stationarity conditions.}
At the unconstrained interior optimum (all $k>0$), the first-order conditions are
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{inv}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa1}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa2}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{xor}}}=0.
\]
Expanding each with the lists above yields five nonlinear equations:
\[
\boxed{\;
\sum_{i=1}^{21}\Big(\sum_{j=1}^{i} \partial R_j/\partial x\Big)C_i
\;+\;\sum_{i=1}^{21} S_i\,\partial C_i/\partial x \;=\;0,
\qquad x\in\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}.
\;}
\]
Concretely, for $x=k_{\text{nand}}$,
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}
=\sum_{i=1}^{21}\!\Bigg(\sum_{j=1}^i\!\left[-\frac{2}{k_{\text{nand}}^2}\,\mathbb{1}_{\{j=2,4\}}\right]\!\Bigg) C_i
\;+\; \sum_{i=1}^{21}\! S_i\,(2\,\mathbb{1}_{\{i=1,3\}})
=0,
\]
and analogous expressions hold for the other variables by substituting their contributing indices and coefficients.

\paragraph{Numerical solution and improvement.}
Solving the five stationarity equations simultaneously for the positive real solution gives the following optimum (continuous sizes, in the normalized units above):
\[
\boxed{
\begin{aligned}
k_{\text{nand}}&\approx 8.27,\quad
k_{\text{inv}}\approx 1.62,\quad
k_{\text{fa1}}\approx 1.79,\\
k_{\text{fa2}}&\approx 1.62,\quad
k_{\text{xor}}\approx 3.15.
\end{aligned}}
\]
With all $k=1$ (baseline), the Elmore delay along CP1 evaluates to
\[
\tau_{\text{base}} \;=\; 4054\ \big[R_{\!un}C_g\big].
\]
At the optimum,
\[
\tau^\* \;=\; 3618.8\ \big[R_{\!un}C_g\big],
\]
which is an \textbf{approx.\ 10.8\%} reduction in the CP1 Elmore constant under the stated assumptions.

\paragraph{Interpretation.}
The optimizer increases $k_{\text{nand}}$ substantially (NANDs appear early and repeatedly), raises $k_{\text{xor}}$ to avoid starving step~11, and keeps the late-stage drivers ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$) moderately above unity to balance the large downstream loads, especially the final $10C_g$. The stationarity equations enforce near-equalized per-stage effort over the 21-step chain, preventing the tail (steps 16--21) from dominating while also avoiding excessive upsizing that would inflate upstream capacitive loads.


\newpage

% ----------------------------------------------------
\section{Optimized Functional Verification}
\label{sec:opt_functional_verification}

The optimized 4$\times$4 multiplier was verified using the same
exhaustive, self-checking infrastructure described for the baseline
design in Section~\ref{sec:functional_verification}. The only change in
the testbench is that the device under test (DUT) is now the optimized
schematic; all stimulus generation, sampling times, and post-processing
scripts are reused without modification. This ensures that any
difference in behavior would be attributable to the circuit changes
rather than to the verification environment.

\subsection{Transient Stimulus and Waveforms}

The eight primary inputs $X_0$--$X_3$ and $Y_0$--$Y_3$ are again driven
by a \texttt{vpulse}-based binary counter with periods
$T,2T,4T,8T$ and $16T,32T,64T,128T$, respectively. The transient
simulation runs for $1.28\,\mu\mathrm{s} = 128T$, so that all
$2^8 = 256$ input combinations $(X,Y)$ with $X,Y \in [0,15]$ are
applied once.

The resulting waveforms for the optimized DUT are shown in
Fig.~\ref{fig:opt_signals}. As in the baseline case, the lower traces
are the input bits and the upper traces are the multiplier outputs
$Z_0$--$Z_7$, which respond deterministically to each new input vector.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/optimized_verification_signals.png}
  \caption{Transient response of the optimized 4$\times$4 multiplier under the same exhaustive binary-count stimulus used for the baseline design.}
  \label{fig:opt_signals}
\end{figure}

\subsection{Digital Reconstruction and Result Table}

The simulator output is exported to CSV and processed by the same pair
of Python scripts used for the baseline:

\begin{itemize}
  \item The first script samples each signal at times
        $t_k = kT/2 + T/4$ (midpoint of every stable interval), converts
        voltages above $1.0\,\mathrm{V}$ to logic ``1'', voltages below
        $0.2\,\mathrm{V}$ to logic ``0'', and flags any intermediate
        values as unsettled.
  \item The second script automatically detects bit ordering from the
        toggle rates, reconstructs the integer operands
        $A$ and $B$, forms the simulated product $Z$, and compares it to
        the ideal product $A \cdot B$ for all $256$ input vectors.
\end{itemize}

For the optimized multiplier the script again reports 256/256 passing
cases; no unsettled outputs are observed at the sampling instants. The
results are visualized in the 16$\times$16 table of
Fig.~\ref{fig:opt_verification_table}, constructed in the same style as
Fig.~\ref{fig:baseline_verification_table}. Each cell corresponds to one
input pair $(X,Y)$, with the simulated product shown in the upper-left
of the cell and the ideal product $X \cdot Y$ in the lower-right; cells
are shaded green when the two numbers agree.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/optimized_verification_visualization.png}
  \caption{Verification table for the optimized 4$\times$4 multiplier. For each input pair $(X,Y)$, the upper-left number is the simulated product and the lower-right number is the ideal product $X \cdot Y$. All 256 cases match, so every cell is green.}
  \label{fig:opt_verification_table}
\end{figure}

Because the optimized implementation passes exactly the same exhaustive
test as the baseline, with identical stimulus and checking criteria, we
can conclude that the optimization preserves the functional behavior of
the multiplier while enabling the delay and energy improvements
discussed in later sections.

\newpage

% ----------------------------------------------------
\section{Optimized Delay Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Active Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Leakage Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Summary of Optimization Process}



\newpage

% ----------------------------------------------------
% ----------------------------------------------------
\section{Design Exploration}
\label{sec:design_exploration}

Although the final implementation uses only complementary static-CMOS gates, we
did explore a range of alternative logic styles and adder structures before
settling on the architecture presented in this report. Our goal was to
demonstrate meaningful design choices \emph{inside} the 4$\times$4 multiplier
project (already more advanced than the alternative 8-bit adder option), without
sacrificing robustness in a tiled, transistor-level implementation.

\subsection{Multiplier Architectures Considered}

At the architectural level we considered three broad multiplier families:

\begin{itemize}
  \item \textbf{Serial (shift-and-add) multipliers}, which reuse a single adder
        across multiple cycles. These are attractive for low-area, low-power
        designs but introduce a multi-cycle control path and are less aligned
        with the ``bit-sliced datapath'' emphasis of the project.
  \item \textbf{Tree-based multipliers} (e.g., Wallace or Dadda trees), which
        use carry-save reduction to shorten the critical path. For 4-bit
        operands, however, the overhead of extra compressor cells and routing is
        comparable to the nominal gain, and the resulting layout is less
        regular.
  \item \textbf{Braun (array) multipliers}, which generate all partial products
        with AND gates and reduce them with a regular grid of HAs and FAs.
        This style has a very regular floorplan and exposes clear critical paths
        (CP$_1$, CP$_2$) that are easy to analyze and optimize.
\end{itemize}

Given the small operand size and the project’s focus on schematic-level cell
design and sizing, we chose the Braun array. It provides enough structure to
make ``critical-path aware'' optimization meaningful, while remaining simple
enough to verify exhaustively at the transistor level.

\subsection{Adder Cell Topologies}

Within the chosen array structure we then considered several full-adder and
half-adder realizations:

\begin{itemize}
  \item A \textbf{pure gate-assembled FA/HA}, built from library-style XOR,
        NAND, NOR, and inverter cells (our baseline). This option is conceptually
        straightforward but produces relatively deep logic chains on the
        critical staircases.
  \item A \textbf{mirror-style static-CMOS FA} (Rabaey-style), in which the
        pull-up and pull-down networks implement the carry logic directly and
        share internal nodes between Sum and Carry. This reduces the number of
        distinct logic stages and internal fanouts.
  \item \textbf{Pass-transistor and transmission-gate adders}, which can reduce
        transistor count and input capacitance at the cost of more complex
        signal-level behavior (level degradation, charge sharing, etc.).
\end{itemize}

We briefly prototyped pass-transistor based XOR and FA variants at the schematic
level, but ultimately focused our optimization effort on the complementary
static-CMOS FA and HA. This choice aligned better with the goal of a robust
tiled array: the static-CMOS cells provide full-swing, monotonic outputs and
simple noise-margin reasoning when cascaded many times.

\subsection{Logic Styles: Static CMOS vs. Ratioed / Pass-Transistor Logic}

During the exploration phase we also considered more aggressive logic styles,
specifically ratioed logic (e.g., pseudo-NMOS) and pass-transistor/transmission
gate networks, as potential ways to speed up the critical path or reduce area.

\paragraph{Ratioed logic.}

Ratioed logic can implement certain functions with fewer transistors than
complementary static CMOS, potentially reducing load capacitance. However, it
comes with several drawbacks that are particularly problematic in this project:

\begin{itemize}
  \item \textbf{Static power consumption}: a permanently on pull-up device
        leads to DC current whenever the pull-down network conducts, which is
        undesirable in a deeply cascaded structure.
  \item \textbf{Reduced noise margins and level restoration requirements}:
        the logic levels depend on device strength ratios; variations in process,
        voltage, or temperature can move outputs away from solid rail-to-rail
        values. Over multiple stages this can accumulate into functional
        uncertainty.
  \item \textbf{Cascading into static-CMOS array cells}: mixing ratioed stages
        with static-CMOS adders along long paths (e.g., CP$_1$ and CP$_2$) makes
        it harder to guarantee that all internal nodes stay in safe regions
        under worst-case switching.
\end{itemize}

Given that our multiplier already tiles many bit-slices and that we verify at
the transistor level rather than with idealized models, we decided not to push
the design further into ratioed logic. The potential delay savings did not
justify the added verification burden and reliability risk in this use case.

\paragraph{Pass-transistor and transmission-gate logic.}

Pass-transistor and transmission-gate structures can be highly efficient for
XOR/XNOR and multiplexing, and many published ``fast adders'' use them. We did
look at such adders, but several issues emerged when mapping them into our
array:

\begin{itemize}
  \item \textbf{Level degradation in NMOS-only passes}: unless explicitly
        followed by restoration inverters, high levels passed through NMOS-only
        networks can degrade by a threshold voltage. This complicates noise
        margin analysis when these nodes drive further CMOS stages.
  \item \textbf{Charge sharing and dynamic behavior}: internal nodes in
        pass-transistor networks can float or experience charge sharing between
        configurations, which is harder to reason about across many cascaded
        cells and worst-case input patterns.
  \item \textbf{Control signal routing and layout complexity}: in a small,
        regular 4$\times$4 array, the control routing overhead for
        transmission-gate structures can offset their transistor-count
        advantage.
\end{itemize}

We concluded that, for this particular project, the verification and robustness
costs of a heavily pass-transistor-based array outweighed the benefits. Since
we were already taking on the more advanced ``4$\times$4 transistor-level
multiplier'' project (rather than the simpler 8-bit adder option), we chose to
focus our design effort on a well-understood, fully static-CMOS style and to
explore optimization via \emph{cell topology} and \emph{transistor sizing}
instead.

\subsection{Final Design Focus}

After this exploration, our final design strategy was:

\begin{itemize}
  \item Keep the \textbf{architecture} simple and regular: a 4$\times$4 Braun
        array with static-CMOS partial-product ANDs, HAs, and FAs.
  \item Use the \textbf{baseline gate-assembled FA/HA} as a functional
        reference, then adopt a \textbf{complementary static-CMOS FA} with
        shared internal nodes as the optimized cell.
  \item Perform \textbf{systematic sizing optimization} along the identified
        critical path (CP$_1$), using an Elmore-delay model and continuous size
        variables to reduce the worst-case delay while keeping the logic style
        robust and fully static.
\end{itemize}

In other words, we did explore more exotic logic families and alternative
architectures but ultimately chose to keep the logic style conservative and
focus our ``advanced'' work on transistor-level optimization and timing
analysis within the static-CMOS 4$\times$4 array framework.




\newpage

% ----------------------------------------------------
\section{Conclusion}



\end{document}
