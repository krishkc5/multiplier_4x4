\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, siunitx, physics}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{caption}

\geometry{margin=1in}

\title{4$\times$4 CMOS Array Multiplier}

%[insert title image]

\author{Krishna Karthikeya Chemudupati \and Adithya Selvakumar}
\date{November 2025}

\begin{document}
\maketitle

\newpage

\tableofcontents

\newpage

% ----------------------------------------------------
\section{Introduction and Background}



\newpage

% ----------------------------------------------------
\section{Baseline Design}



\newpage

% ----------------------------------------------------
\section{Baseline Functional Verification}

To verify the correctness of both the baseline and optimized 4$\times$4 multiplier implementations, we built a fully exhaustive, self-checking testbench around the transistor-level schematic. The testbench (1) applies all $2^8 = 256$ possible input combinations to the multiplier, (2) samples the eight output bits only after they have settled, and (3) automatically checks that every sampled output word matches the ideal product $Z = X \cdot Y$.

\subsection{Exhaustive Transient Stimulus}

The testbench consists of the multiplier under test driven by eight independent \texttt{vpulse} sources, one per primary input bit ($X_0$–$X_3$ and $Y_0$–$Y_3$). Each source is configured with a different period so that, taken together, the inputs realize a hardware binary counter:

\begin{center}
\begin{tabular}{c c}
$X_0$: period $T = 10\,\mathrm{ns}$, & $Y_0$: period $16T$ \\
$X_1$: period $2T$,                  & $Y_1$: period $32T$ \\
$X_2$: period $4T$,                  & $Y_2$: period $64T$ \\
$X_3$: period $8T$,                  & $Y_3$: period $128T$
\end{tabular}
\end{center}

With a 50\% duty cycle on each bit, this configuration causes the eight inputs to count through all binary patterns from 0 to 255. Over a total simulation time of $1.28\,\mu\mathrm{s} = 128T$, the testbench therefore applies every possible unsigned input pair $(X,Y)$ with $X,Y \in [0,15]$. The corresponding Cadence waveform capture is shown in Fig.~\ref{fig:baseline_signals}, where the lower traces are the input bits and the upper traces are the multiplier outputs $Z_0$–$Z_7$.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/baseline_verification_signals.png}
  \caption{Transient response of the 4$\times$4 multiplier under exhaustive binary-count stimulus. The eight input bits form a hardware counter, and the eight output bits respond to each new input vector.}
  \label{fig:baseline_signals}
\end{figure}

Because this verification runs on the full transistor-level schematic, it naturally captures the effects of propagation delay, glitching, and finite output slew; no behavioral or idealized models are used.

\subsection{Sampling and Logic-Level Conversion}

The simulator output is exported to CSV and processed in Python. Rather than sampling at arbitrary times, we choose one time point in the middle of each interval where the inputs are guaranteed to be static. The shortest time between two transitions on any input is $T/2$ (the high or low half of $X_0$'s period), so we define the $k$th sampling instant as
\[
t_k = \frac{kT}{2} + \frac{T}{4}, \qquad k = 0,1,\dots,255.
\]
At $t_k$ the counter has been stable for at least $T/4$ and the multiplier output has had time to settle to its final value.

For each $t_k$, the script locates the nearest time point in the exported CSV and extracts the voltages for all input and output nodes. These analog voltages are then converted to logic levels using conservative thresholds:
\[
V > 1.0\,\mathrm{V} \Rightarrow \text{logic 1}, \qquad
V < 0.2\,\mathrm{V} \Rightarrow \text{logic 0}.
\]
Values that fall between 0.2\,V and 1.0\,V are marked as ``unsettled'' (stored as NaN) instead of being forced to a digital 0 or 1. This makes the check robust to slow transitions or ringing: if any output bit has not reached a valid logic level by the sampling time, the issue is surfaced immediately in the processed data rather than silently misclassified.

The result of this first Python stage is a new CSV file with exactly 256 rows (one per input vector) and clean digital values for all bits $X_3$–$X_0$, $Y_3$–$Y_0$, and $Z_7$–$Z_0$.

\subsection{Reconstruction of Integer Operands and Products}

A second Python script interprets the bit fields as integers and checks the arithmetic. A practical concern when exporting from Cadence is that the bit order in the CSV may not match the expected LSB-to-MSB ordering. To guard against this, the script first analyzes the number of transitions on each bit; the bit with the highest toggle count must be the LSB (it has the shortest period). If the fastest-toggling column is labeled $X_3$ instead of $X_0$, for example, the script concludes that the bits are reversed and flips the order in software before proceeding. The same logic is applied to the $Y$ inputs. This automatic detection makes the verification insensitive to column-ordering mistakes in the export step.

With the bit ordering corrected, the integer values are reconstructed as
\begin{align*}
A &= X_0 + 2X_1 + 4X_2 + 8X_3, \\
B &= Y_0 + 2Y_1 + 4Y_2 + 8Y_3, \\
Z &= Z_0 + 2Z_1 + 4Z_2 + 8Z_3 + 16Z_4 + 32Z_5 + 64Z_6 + 128Z_7.
\end{align*}
These become the columns \texttt{A\_val}, \texttt{B\_val}, and \texttt{Z\_val}. The correct reference result is simply
\[
\texttt{Expected} = \texttt{A\_val} \times \texttt{B\_val}.
\]
For each of the 256 vectors the script sets a Boolean flag \texttt{Pass} if \texttt{Z\_val == Expected}. A short summary printed at the end of the run reports how many of the 256 cases passed and, if any failed, lists the mismatched vectors with their input pair, measured product, and expected product. For both the baseline and optimized multipliers, the script reports 256/256 passing cases.

\subsection{Verification Table Visualization}

To make the results easy to interpret at a glance, a final Python script converts the verified dataset into the 16$\times$16 table shown in Fig.~\ref{fig:baseline_verification_table}. The horizontal axis corresponds to the integer value of $X$, the vertical axis corresponds to $Y$, and each cell encodes the product for one $(X,Y)$ pair:

\begin{itemize}
  \item The upper-left number in each cell is the measured product $Z$ from the simulation.
  \item The lower-right number is the ideal product $X \cdot Y$.
  \item The cell background is colored green if these two numbers agree and red otherwise.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/baseline_verification_visualization.png}
  \caption{Visualization of functional verification for the 4$\times$4 multiplier. For each input pair $(X,Y)$, the upper-left number is the simulated product and the lower-right number is the ideal product $X \cdot Y$. Matching cells are shaded green; any mismatch would appear as a red cell.}
  \label{fig:baseline_verification_table}
\end{figure}

Because the multiplier is correct, every cell in Fig.~\ref{fig:baseline_verification_table} is green and the two numbers in each cell are identical. Any systematic wiring error (for example, swapping two output bits or mis-interpreting signedness) would create a visible pattern of red cells, making this visualization a powerful sanity check in addition to the numerical pass/fail summary. The same scripts are reused for both the baseline and optimized designs, so any changes to the circuit can be re-verified quickly and objectively using identical criteria.

\newpage

% ----------------------------------------------------
\section{Baseline Delay Measurement}



\newpage

% ----------------------------------------------------
\section{Baseline Active Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Baseline Leakage Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Design}

\subsection{Worst Case Delay}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\linewidth]{writeup/figures/arraymult_criticalpath.png}
  \caption{Critical-path staircases in a 4×4 Braun array multiplier. X$_0$–X$_3$ are the N bits (multiplicand) and Y$_0$–Y$_3$ are the M bits (multiplier). Figure adapted from materials by Prof. Janakiraman Viraraghavan, IIT Madras.}
\end{figure}

\noindent
We analyze the 4×4 Braun (array) multiplier shown above. The multiplicand bits are X$_0$–X$_3$ (the N bits) and the multiplier bits are Y$_0$–Y$_3$ (the M bits). Each partial product x$_i$y$_j$ is generated by a static-CMOS AND gate, and these partial products are reduced by a regular grid of half adders (HAs) and full adders (FAs). Within a column, carries propagate downward; between columns, sums propagate leftward. The slow paths are the northeast-to-southwest staircases that begin at a right-edge partial product and alternate vertical carry hops with horizontal sum hops until they reach the lower-left outputs.

\paragraph{Delay primitives and convention}
We define three cell delays under the loading and drive conditions that match the tiled array:
\[
t_{\text{and}} \text{ for the partial-product AND,}\quad
t_{\text{carry}} \text{ for an HA/FA carry-out,}\quad
t_{\text{sum}} \text{ for an HA/FA sum-out.}
\]
In static-CMOS adders the sum network is built from XOR/XNOR structures that have larger logical effort and parasitic capacitance than the carry (majority / AND-OR) network, so typically
\[
t_{\text{sum}} > t_{\text{carry}}.
\]

\paragraph{Exciting the limiting staircase}
The worst transition is not the input vector that flips the most bits. It is the transition that (i) toggles a right-edge partial product and (ii) places each encountered adder in propagate mode (a$\oplus$b = 1 and ab = 0), forcing a newly injected carry to ripple through all stages along a staircase to the lower-left.

\paragraph{Counting hops on an M×N array}
For any such staircase in an M×N array, the stage counts are:
\[
\begin{aligned}
\text{carry hops}   &= (M-1) + (N-2),\\
\text{sum hops}     &= (M-1),
\end{aligned}
\]
plus the initial $t_{\text{and}}$ that launches the path. Therefore the generic array-multiplier timing is
\[
t_{\text{array}}(M,N) \;=\; \big[(M-1) + (N-2)\big]\,t_{\text{carry}}
\;+\; (M-1)\,t_{\text{sum}}
\;+\; t_{\text{and}}.
\]

\paragraph{Specialization to the 4×4 case}
With $M=N=4$ we obtain five carry hops and three sum hops:
\[
t_{\text{path}} \;=\; 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}.
\]
In the 4×4 topology the bottom-left FA drives two neighboring outputs: its carry is Z$_7$ and its sum is Z$_6$. The staircase counted above terminates at the sum of this FA, so
\[
\boxed{\,t_{Z_6} = 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}\,}.
\]
Because $t_{\text{sum}} > t_{\text{carry}}$ for static-CMOS FAs, Z$_6$ is slower than the otherwise identical staircase that would end at the carry output Z$_7$. Hence the worst-case delay of the 4×4 array ends at Z$_6$, not Z$_7$.

\paragraph{Implications}
Multiple staircases (such as CP$_1$ and CP$_2$ in the figure) have the same hop counts and are nearly iso-delay, so cell sizing must balance stage effort across the grid. Nevertheless, the decisive last stage is the sum network of the bottom-left FA that produces Z$_6$, and this stage should be weighted accordingly when sizing to minimize the overall worst-case delay.

\subsection{Schematics in Static CMOS}
\subsubsection{Full Adder (FA)}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{writeup/figures/staticcmos_fa_rabaey_annotated.png}
  \caption{Complementary static-CMOS full adder. Figure adapted from Rabaey.}
\end{figure}

\noindent
The full adder computes the carry-out $C_o$ and sum $S$ from inputs $A$, $B$, and $C_i$.  
We employ a complementary static-CMOS realization that shares intermediate logic between
the carry and sum paths. The logic is organized as
\[
C_o = AB + BC_i + AC_i \quad\text{(3-input majority)}, \qquad
S = ABC_i + C_o\,(A + B + C_i).
\]
The schematic implements these expressions with series–parallel $p$- and $n$-networks so that
each output is produced by static pull-up and pull-down paths, guaranteeing full-swing,
monotonic transitions and large noise margins.

\paragraph{Signal flow and stage counts}
The circuit is partitioned around an internal node $X$ that realizes the reorganized carry logic.
\begin{itemize}
  \item \textbf{Path to $C_o$ (two static stages).} A complex CMOS network first forms $X$,
  which captures the majority function. An output inverter then buffers $X$ to produce $C_o$.
  \item \textbf{Path to $S$ (three static stages).} The same $X$ is reused by a second
  static network that combines $X$ with $A$, $B$, and $C_i$ to generate a pre-sum node.
  A final inverter buffers this node to produce $S$.
\end{itemize}
Thus, $C_o$ emerges after two static stages and $S$ after three. While not ``single-stage''
at the pins, these are compact paths with low internal fanout and limited parasitic loading.

\paragraph{Why static CMOS over a bag of gates}
A gate-assembled FA (e.g., library XOR/AND/OR composition) typically yields a deeper chain:
$C_o$ commonly requires two XOR-class stages, and $S$ often traverses a sequence such as
XOR $\rightarrow$ NAND $\rightarrow$ NOT $\rightarrow$ NOR $\rightarrow$ NOT.
Each additional logic boundary adds logical effort and parasitic capacitance, inflating
end-to-end delay and degrading slews. The static-CMOS realization reduces the number of
boundaries, shares $X$ rather than duplicating heavy networks, and eliminates large internal
fanouts between discrete gates. Because both outputs are produced by complementary static
networks and then buffered, signals remain rail-to-rail and monotonic across many cascaded cells.

\paragraph{Fit for the array multiplier}
In the 4$\times$4 Braun array, the slow paths are the northeast-to-southwest staircases of adders,
and the measured worst case terminates at the sum output of the bottom-left FA ($Z_6$).
Replacing multi-gate chains with the compact static implementation shortens the effective
stage depth on these staircases and reduces internal parasitics, lowering the overall delay.

\noindent At the same time, static-CMOS robustness (no ratioing, no dynamic storage) ensures predictable
behavior when many FA cells are tiled: outputs do not droop through the grid, hazards are
suppressed by the static topologies and output inverters, and uniform polarity avoids the need
for multiple FA variants or extra inverters during integration.

\noindent
In summary, the Rabaey complementary static-CMOS FA provides a favorable speed–robustness tradeoff
for the multiplier: fewer and better stages than a bag-of-gates implementation, shared intermediate
logic that limits internal loading, and full-swing stability that holds when cascading many cells.

\subsubsection{Half Adder (HA)}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_xor.png}\hfill
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_nand.png}
  \caption{Static-CMOS XOR (left) for Sum and NAND (right) used with a following inverter for Carry. Figure adapted from Shakshat Virtual Lab, IIT Guwahati}
\end{figure}

\noindent
The half adder takes one-bit inputs $A$ and $B$ and produces outputs \textit{Sum} and \textit{Carry}. From the truth table,
\[
\text{Sum} = A \oplus B,
\qquad
\text{Carry} = A \cdot B = \overline{\mathrm{NAND}(A,B)}.
\]

\noindent
Implementation in complementary static CMOS is therefore direct:
\begin{itemize}
  \item \textbf{Sum path.} Realize $A \oplus B$ with a static-CMOS XOR gate. This is a single logic stage at the Sum pin with full-swing, monotonic behavior.
  \item \textbf{Carry path.} Realize $A \cdot B$ as a static-CMOS NAND followed by a static inverter: $\text{Carry}=\overline{\mathrm{NAND}(A,B)}$. This is two logic stages at the Carry pin (NAND then inverter), also full swing and monotonic.
\end{itemize}

\noindent
Unlike the full adder, there is no third input to exploit for algebraic sharing or logic reorganization; the minimal Boolean forms are already $A \oplus B$ and $A \cdot B$. Consequently, the “optimized” static-CMOS HA coincides with the baseline bag-of-gates realization: one XOR stage for Sum and a NAND+inverter pair for Carry. This keeps logic depth minimal for each output while retaining the robustness advantages of static CMOS (rail-to-rail levels, large noise margins, and predictable cascading behavior in the multiplier array).

\subsection{Optimizing Gate Sizes Along CP1}

\paragraph{Design variables and device mapping.}
We size five gate types with continuous, positive scale factors
\[
k_{\text{nand}},\;k_{\text{inv}},\;k_{\text{fa1}},\;k_{\text{fa2}},\;k_{\text{xor}}>0.
\]
For the \textbf{NAND} gate, both PMOS and NMOS widths equal $k_{\text{nand}}$.
For the \textbf{INV}, \textbf{FA Stage~1}, \textbf{FA Stage~2}, and \textbf{XOR}, the PMOS width is $2k$ and the NMOS width is $k$, where $k$ is the corresponding scale ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$, $k_{\text{xor}}$).

\paragraph{Assumptions and units.}
We assume: (i) diffusion capacitances are neglected ($C_d=0$);
(ii) all inputs are driven by a minimum-size inverter; (iii) pull-up resistance $R_{\!up}=2R_{\!un}$;
(iv) the final output load is $10C_g$. We measure delay in units of $R_{\!un}C_g$; i.e., we factor out the common multiplier $R_{\!un}C_g$ from every $R_i C_j$ product.

\paragraph{Definition of CP1.}
The critical path instance CP1 crosses the following gate sequence:
\[
\begin{aligned}
&\text{AND }(\text{NAND}+\text{INV});\ 
\text{HA,carry }(\text{NAND}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\\
&\text{HA,sum }(\text{INV}+\text{XOR});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV});\\
&\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV}).
\end{aligned}
\]

\paragraph{Step-by-step RC model (21 steps).}
Let $(R_i, C_i)$ denote the driving resistance and the load capacitance of step $i$ on CP1, each expressed in the normalized units described above. Using the worst-case edge per step you specified:

\[
\begin{aligned}
R_1&=2,                      & C_1&=2k_{\text{nand}}, \\
R_2&=\frac{2}{k_{\text{nand}}}, & C_2&=3k_{\text{inv}}, \\
R_3&=\frac{2}{k_{\text{inv}}},   & C_3&=2k_{\text{nand}}, \\
R_4&=\frac{2}{k_{\text{nand}}}, & C_4&=3k_{\text{inv}}, \\
R_5&=\frac{2}{k_{\text{inv}}},   & C_5&=6k_{\text{fa1}}, \\
R_6&=\frac{6}{k_{\text{fa1}}},   & C_6&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_7&=\frac{2}{k_{\text{inv}}},   & C_7&=6k_{\text{fa1}}, \\
R_8&=\frac{6}{k_{\text{fa1}}},   & C_8&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_9&=\frac{2}{k_{\text{inv}}},   & C_9&=3k_{\text{inv}}, \\
R_{10}&=\frac{2}{k_{\text{inv}}},& C_{10}&=3k_{\text{xor}}, \\
R_{11}&=\frac{4}{k_{\text{xor}}},& C_{11}&=6k_{\text{fa1}}, \\
R_{12}&=\frac{6}{k_{\text{fa1}}},& C_{12}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{13}&=\frac{2}{k_{\text{inv}}},& C_{13}&=6k_{\text{fa1}}, \\
R_{14}&=\frac{6}{k_{\text{fa1}}},& C_{14}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{15}&=\frac{4}{k_{\text{fa2}}},& C_{15}&=3k_{\text{inv}}, \\
R_{16}&=\frac{2}{k_{\text{inv}}},& C_{16}&=6k_{\text{fa1}}, \\
R_{17}&=\frac{6}{k_{\text{fa1}}},& C_{17}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{18}&=\frac{2}{k_{\text{inv}}},& C_{18}&=6k_{\text{fa1}}, \\
R_{19}&=\frac{6}{k_{\text{fa1}}},& C_{19}&=3k_{\text{inv}}+3k_{\text{fa2}}, \\
R_{20}&=\frac{4}{k_{\text{fa2}}},& C_{20}&=3k_{\text{inv}}, \\
R_{21}&=\frac{2}{k_{\text{inv}}},& C_{21}&=10. \\
\end{aligned}
\]

\paragraph{Elmore delay objective.}
Let the cumulative resistance up to step $i$ be
\[
S_i \;\triangleq\; \sum_{j=1}^{i} R_j.
\]
The Elmore delay along CP1 is
\[
\tau(k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}})
= \sum_{i=1}^{21} S_i\,C_i.
\]
By construction, $\tau$ is homogeneous in the time unit $R_{\!un}C_g$.

\paragraph{Structure of the derivatives.}
Each $R_i$ is either constant or of the form $\alpha_i/x$ for a single sizing variable $x\in
\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}$; each $C_i$ is either constant or of the form $\beta_i x$ or a sum of such linear terms. Hence
\[
\frac{\partial R_i}{\partial x}=
\begin{cases}
-\dfrac{\alpha_i}{x^2}, & \text{if } R_i=\dfrac{\alpha_i}{x},\\[6pt]
0, & \text{otherwise},
\end{cases}
\qquad
\frac{\partial C_i}{\partial x}=
\begin{cases}
\beta_i, & \text{if } C_i\text{ contains }\beta_i x,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Using $S_i=\sum_{j\le i}R_j$, its derivative satisfies
\[
\frac{\partial S_i}{\partial x}=\sum_{j=1}^{i}\frac{\partial R_j}{\partial x}.
\]
Therefore the gradient components are
\[
\boxed{\quad
\frac{\partial \tau}{\partial x}
= \sum_{i=1}^{21}\left(\frac{\partial S_i}{\partial x}\,C_i
+ S_i\,\frac{\partial C_i}{\partial x}\right)
= \sum_{i=1}^{21}\left(\sum_{j=1}^i \frac{\partial R_j}{\partial x}\right)C_i
+ \sum_{i=1}^{21} S_i\,\frac{\partial C_i}{\partial x}.
\quad}
\]

\paragraph{Coefficient bookkeeping per variable.}
For compactness, we list the steps that contribute to each variable’s partial derivative, with their coefficients $(\alpha,\beta)$:

\begin{itemize}
\item \textbf{$x=k_{\text{nand}}$:}
\[
\begin{aligned}
&\text{Resistance terms: } R_2=\tfrac{2}{k_{\text{nand}}},\ R_4=\tfrac{2}{k_{\text{nand}}}
\ \Rightarrow\ \alpha_{2}=\alpha_{4}=2;\\
&\text{Capacitance terms: } C_1=2k_{\text{nand}},\ C_3=2k_{\text{nand}}
\ \Rightarrow\ \beta_{1}=\beta_{3}=2.
\end{aligned}
\]

\item \textbf{$x=k_{\text{inv}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_3,R_5,R_7,R_9,R_{10},R_{13},R_{16},R_{18},R_{21}
= \tfrac{2}{k_{\text{inv}}}\ \Rightarrow\ \alpha=2\ \text{at those indices};\\
&\text{Capacitance: } C_2,C_4,C_6,C_8,C_9,C_{12},C_{14},C_{15},C_{17},C_{19},C_{20}
= 3k_{\text{inv}}\ \Rightarrow\ \beta=3\ \text{at those indices}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa1}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_6,R_8,R_{12},R_{14},R_{17},R_{19}=\tfrac{6}{k_{\text{fa1}}}
\ \Rightarrow\ \alpha=6\ \text{at those indices};\\
&\text{Capacitance: } C_5,C_7,C_{11},C_{13},C_{16},C_{18}=6k_{\text{fa1}},\\
&\hspace*{25mm}
C_6,C_8,C_{12},C_{14},C_{17},C_{19}\ \text{contain }3k_{\text{fa1}}
\ \Rightarrow\ \beta=6\ \text{or }3\ \text{accordingly}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa2}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_{15},R_{20}=\tfrac{4}{k_{\text{fa2}}}
\ \Rightarrow\ \alpha=4\ \text{at }15,20;\\
&\text{Capacitance: } C_{19}=3k_{\text{fa2}}
\ \Rightarrow\ \beta_{19}=3.
\end{aligned}
\]

\item \textbf{$x=k_{\text{xor}}$:}
\[
\text{Resistance: } R_{11}=\tfrac{4}{k_{\text{xor}}}\ (\alpha_{11}=4),\qquad
\text{Capacitance: } C_{10}=3k_{\text{xor}}\ (\beta_{10}=3).
\]
\end{itemize}

\paragraph{Stationarity conditions.}
At the unconstrained interior optimum (all $k>0$), the first-order conditions are
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{inv}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa1}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa2}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{xor}}}=0.
\]
Expanding each with the lists above yields five nonlinear equations:
\[
\boxed{\;
\sum_{i=1}^{21}\Big(\sum_{j=1}^{i} \partial R_j/\partial x\Big)C_i
\;+\;\sum_{i=1}^{21} S_i\,\partial C_i/\partial x \;=\;0,
\qquad x\in\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}.
\;}
\]
Concretely, for $x=k_{\text{nand}}$,
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}
=\sum_{i=1}^{21}\!\Bigg(\sum_{j=1}^i\!\left[-\frac{2}{k_{\text{nand}}^2}\,\mathbb{1}_{\{j=2,4\}}\right]\!\Bigg) C_i
\;+\; \sum_{i=1}^{21}\! S_i\,(2\,\mathbb{1}_{\{i=1,3\}})
=0,
\]
and analogous expressions hold for the other variables by substituting their contributing indices and coefficients.

\paragraph{Numerical solution and improvement.}
Solving the five stationarity equations simultaneously for the positive real solution gives the following optimum (continuous sizes, in the normalized units above):
\[
\boxed{
\begin{aligned}
k_{\text{nand}}&\approx 8.27,\quad
k_{\text{inv}}\approx 1.62,\quad
k_{\text{fa1}}\approx 1.79,\\
k_{\text{fa2}}&\approx 1.62,\quad
k_{\text{xor}}\approx 3.15.
\end{aligned}}
\]
With all $k=1$ (baseline), the Elmore delay along CP1 evaluates to
\[
\tau_{\text{base}} \;=\; 4054\ \big[R_{\!un}C_g\big].
\]
At the optimum,
\[
\tau^\* \;=\; 3618.8\ \big[R_{\!un}C_g\big],
\]
which is an \textbf{approx.\ 10.8\%} reduction in the CP1 Elmore constant under the stated assumptions.

\paragraph{Interpretation.}
The optimizer increases $k_{\text{nand}}$ substantially (NANDs appear early and repeatedly), raises $k_{\text{xor}}$ to avoid starving step~11, and keeps the late-stage drivers ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$) moderately above unity to balance the large downstream loads, especially the final $10C_g$. The stationarity equations enforce near-equalized per-stage effort over the 21-step chain, preventing the tail (steps 16--21) from dominating while also avoiding excessive upsizing that would inflate upstream capacitive loads.


\newpage

% ----------------------------------------------------
\section{Optimized Functional Verification}



\newpage

% ----------------------------------------------------
\section{Optimized Delay Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Active Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Leakage Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Summary of Optimization Process}



\newpage

% ----------------------------------------------------
\section{Summary Comparison of Baseline and Optimized Designs}



\newpage

% ----------------------------------------------------
\section{Conclusion}



\end{document}
