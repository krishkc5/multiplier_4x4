\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, siunitx, physics}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{caption}

\geometry{margin=1in}

\title{\textbf{4 $\times$ 4 CMOS Array Multiplier} \\}
       \vspace{0.5cm}

\author{Krishna Karthikeya Chemudupati \\ Adithya Selvakumar \\}
\date{November 13, 2025}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage

\tableofcontents

\newpage

% ----------------------------------------------------
\section{Introduction and Background}



\newpage

% ----------------------------------------------------
% ----------------------------------------------------
\section{Baseline Design}
\label{sec:baseline_design}

The baseline implementation is a fully static-CMOS 4$\times$4 Braun (array) multiplier
constructed hierarchically from \emph{minimum-sized} logic gates in the 45\,nm, 1.2\,V
process specified in the project handout. In this baseline, every MOSFET
(NMOS and PMOS) uses the minimum width and minimum channel length allowed by the PDK;
no cell-level upsizing or skew tuning is performed.

At the top level, the multiplier is built from:

\begin{itemize}
  \item sixteen 2-input AND gates that generate the partial products $X_i Y_j$,
  \item a grid of half adders (HAs) and full adders (FAs) that sum the partial products, and
  \item wiring that routes carries downward and sums leftward, as in a standard Braun array.
\end{itemize}

All HAs and FAs are themselves built from library-style XOR, NAND, NOR, AND, and
inverter cells, each implemented as a minimum-size static-CMOS gate. The overall structure
and the corresponding schematics are shown in the figures below.

\subsection{Array-Level Architecture}

Figure~\ref{fig:baseline_arch} shows the gate-level structure of the baseline 4$\times$4
multiplier. The multiplicand bits $X_0$--$X_3$ are placed along the top of the array, and
the multiplier bits $Y_0$--$Y_3$ enter from the right. Each AND gate at the top row
computes a partial product $X_i Y_j$, and these partial products feed into the HA/FA
array below.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{writeup/figures/baseline_multiplier.png}
  \caption{Gate-level structure of the baseline 4$\times$4 Braun array multiplier. Partial
  products $X_i Y_j$ are generated by AND gates and reduced by a regular grid of full adders
  (FA) and half adders (HA) to produce the outputs $Z_0$--$Z_7$.}
  \label{fig:baseline_arch}
\end{figure}

The least significant output bit $Z_0$ is simply the partial product $X_0 Y_0$. Higher-order
bits are formed by accumulating multiple partial products and carries:

\begin{itemize}
  \item Column $Z_1$ adds $X_1 Y_0$ and $X_0 Y_1$ with a half adder.
  \item Intermediate columns (e.g., $Z_2$, $Z_3$, $Z_4$) combine three or more terms using
        one HA at the top followed by FAs underneath.
  \item The most significant bits $Z_6$ and $Z_7$ are formed at the bottom-left corner by
        the last FA and its carry-out.
\end{itemize}

Within each column, carries propagate \emph{downward} from one adder to the next; between
columns, sums propagate \emph{leftward}. This produces the characteristic
northeast-to-southwest ``staircase'' critical paths (CP$_1$, CP$_2$) used in the worst-case
delay analysis.

\subsection{Baseline Adder Cells}

The baseline array uses gate-assembled HAs and FAs. Their schematics are shown in
Fig.~\ref{fig:baseline_adders}. Every gate instance in these schematics is built from
minimum-size transistors; there is no sizing difference between baseline and optimized
implementations at this level.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.95\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/full_adder.png}
    \caption{Baseline gate-level full adder (FA). The Sum path is implemented with two
    cascaded XOR2 cells; the Carry path uses NAND2, INV, and NOR2 stages followed
    by a final inverter.}
    \label{fig:baseline_fa}
  \end{subfigure}

  \vspace{1em}

  \begin{subfigure}{0.7\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/half_adder.png}
    \caption{Baseline gate-level half adder (HA) built from an XOR2 (Sum) and a NAND2
    followed by an inverter (Carry).}
    \label{fig:baseline_ha}
  \end{subfigure}

  \caption{Adder cells used in the baseline 4$\times$4 array multiplier.}
  \label{fig:baseline_adders}
\end{figure}

\subsubsection*{Full adder}

The full adder implements the usual three-input addition:
\[
  S = A \oplus B \oplus C_{\text{in}},
  \qquad
  C_{\text{out}} = AB + C_{\text{in}}(A \oplus B).
\]

In the baseline schematic (Fig.~\ref{fig:baseline_fa}) this is realized as:

\begin{itemize}
  \item \textbf{Sum path}: two cascaded XOR2 gates. The first XOR computes $A \oplus B$;
        the second XOR combines this result with $C_{\text{in}}$ to generate $S$.
  \item \textbf{Carry path}: a small network of NAND2, INV, and NOR2 gates. A NAND2
        followed by an inverter generates $AB$. In parallel, $A \oplus B$ and $C_{\text{in}}$
        are combined so that the final NOR2+INV sequence implements
        $C_{\text{out}} = AB + C_{\text{in}}(A \oplus B)$.
\end{itemize}

This ``bag-of-gates'' implementation is conceptually simple and easy to map from the Boolean
expressions, but it introduces multiple logic stages on both the Sum and Carry paths. In
particular, the Carry output passes through several cascaded gates
(NAND2 $\rightarrow$ INV $\rightarrow$ NOR2 $\rightarrow$ INV), which contributes to the
relatively large delay of the baseline design along the critical staircases of the array.

\subsubsection*{Half adder}

The half adder computes
\[
  \text{Sum} = A \oplus B, \qquad \text{Carry} = A \cdot B.
\]

As shown in Fig.~\ref{fig:baseline_ha}, the Sum output is generated directly by a single
XOR2 gate, while the Carry output is realized as a NAND2 followed by an inverter
($\text{Carry} = \overline{\text{NAND}(A,B)}$). This two-gate sequence yields a static-CMOS
implementation of the AND function. Compared to the full adder, the HA has shallower
logic depth, which is why it is placed at the top of each column, where it only needs to add
two partial products and does not receive a carry input.

\subsection{Primitive CMOS Gate Implementations}

All logic gates in the baseline adder cells are implemented as transistor-level static-CMOS
cells using \emph{minimum-size} devices. Figure~\ref{fig:baseline_gates} shows the
schematics of the XOR2, NOR2, NAND2, AND2, and inverter cells used throughout the design.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/XOR2.png}
    \caption{XOR2}
    \label{fig:gate_xor2}
  \end{subfigure}\hfill
  \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/NOR2.png}
    \caption{NOR2}
    \label{fig:gate_nor2}
  \end{subfigure}\hfill
  \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/NAND2.png}
    \caption{NAND2}
    \label{fig:gate_nand2}
  \end{subfigure}

  \vspace{1em}

  \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/AND2.png}
    \caption{AND2 (NAND2 + INV)}
    \label{fig:gate_and2}
  \end{subfigure}\hfill
  \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/INV.png}
    \caption{Inverter}
    \label{fig:gate_inv}
  \end{subfigure}

  \caption{Primitive static-CMOS logic cells used in the baseline design. All transistors are
  minimum-sized devices (minimum width and length) in the 45\,nm, 1.2\,V process.}
  \label{fig:baseline_gates}
\end{figure}

\paragraph{Inverter.}
The inverter (\subref{fig:gate_inv}) is a standard complementary pair: one PMOS pull-up to
$V_{\mathrm{DD}}$ and one NMOS pull-down to ground, both using the minimum device size
allowed by the PDK. No skew optimization is performed; this cell serves as the baseline
reference driver and as the restoration stage after NAND2 to form an AND gate.

\paragraph{NAND2 and AND2.}
The 2-input NAND gate (\subref{fig:gate_nand2}) uses two series NMOS devices in the
pull-down network and two parallel PMOS devices in the pull-up network, all minimum size.
Its output is low only when both inputs are high. The AND function is implemented as
a NAND2 followed by an inverter (\subref{fig:gate_and2}), providing a full-swing static-CMOS
realization of $A\cdot B$ used for the HA Carry and within the FA carry network.

\paragraph{NOR2.}
The 2-input NOR gate (\subref{fig:gate_nor2}) is the dual of NAND2: two parallel NMOS
devices in the pull-down and two series PMOS devices in the pull-up, again with minimum
sizes. It outputs logic high only when both inputs are low and is used in the FA
carry-combine network.

\paragraph{XOR2.}
The XOR2 cell (\subref{fig:gate_xor2}) implements $A\oplus B$ using series/parallel
transistor networks for the pull-up and pull-down, so that the output is high exactly when
$A$ and $B$ differ. This gate is the critical building block for the Sum outputs of both the
FA and HA and therefore appears frequently along timing-critical paths in the array.

\subsection{Summary of Baseline Structure}

In summary, the baseline design is a straightforward Braun array built from gate-level HAs
and FAs, which in turn are constructed from minimum-sized static-CMOS logic cells. The
architecture is regular and easy to reason about, but the gate-assembled adders lead to
relatively deep logic chains, especially on the carry paths. This baseline provides a clean,
fully functional reference against which we later compare the optimized full-adder topology
and critical-path sizing results.

\newpage

% ----------------------------------------------------
\section{Baseline Functional Verification}
\label{sec:functional_verification}

To verify the correctness of both the baseline and optimized 4$\times$4 multiplier implementations, we built a fully exhaustive, self-checking testbench around the transistor-level schematic. The testbench (1) applies all $2^8 = 256$ possible input combinations to the multiplier, (2) samples the eight output bits only after they have settled, and (3) automatically checks that every sampled output word matches the ideal product $Z = X \cdot Y$.

\subsection{Exhaustive Transient Stimulus}

The testbench consists of the multiplier under test driven by eight independent \texttt{vpulse} sources, one per primary input bit ($X_0$–$X_3$ and $Y_0$–$Y_3$). Each source is configured with a different period so that, taken together, the inputs realize a hardware binary counter:

\begin{center}
\begin{tabular}{c c}
$X_0$: period $T = 10\,\mathrm{ns}$, & $Y_0$: period $16T$ \\
$X_1$: period $2T$,                  & $Y_1$: period $32T$ \\
$X_2$: period $4T$,                  & $Y_2$: period $64T$ \\
$X_3$: period $8T$,                  & $Y_3$: period $128T$
\end{tabular}
\end{center}

With a 50\% duty cycle on each bit, this configuration causes the eight inputs to count through all binary patterns from 0 to 255. Over a total simulation time of $1.28\,\mu\mathrm{s} = 128T$, the testbench therefore applies every possible unsigned input pair $(X,Y)$ with $X,Y \in [0,15]$. The corresponding Cadence waveform capture is shown in Fig.~\ref{fig:baseline_signals}, where the lower traces are the input bits and the upper traces are the multiplier outputs $Z_0$–$Z_7$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/baseline_verification_signals.png}
  \caption{Transient response of the 4$\times$4 multiplier under exhaustive binary-count stimulus. The eight input bits form a hardware counter, and the eight output bits respond to each new input vector.}
  \label{fig:baseline_signals}
\end{figure}

Because this verification runs on the full transistor-level schematic, it naturally captures the effects of propagation delay, glitching, and finite output slew; no behavioral or idealized models are used.

\subsection{Sampling and Logic-Level Conversion}

The simulator output is exported to CSV and processed in Python. Rather than sampling at arbitrary times, we choose one time point in the middle of each interval where the inputs are guaranteed to be static. The shortest time between two transitions on any input is $T/2$ (the high or low half of $X_0$'s period), so we define the $k$th sampling instant as
\[
t_k = \frac{kT}{2} + \frac{T}{4}, \qquad k = 0,1,\dots,255.
\]
At $t_k$ the counter has been stable for at least $T/4$ and the multiplier output has had time to settle to its final value.

For each $t_k$, the script locates the nearest time point in the exported CSV and extracts the voltages for all input and output nodes. These analog voltages are then converted to logic levels using conservative thresholds:
\[
V > 1.0\,\mathrm{V} \Rightarrow \text{logic 1}, \qquad
V < 0.2\,\mathrm{V} \Rightarrow \text{logic 0}.
\]
Values that fall between 0.2\,V and 1.0\,V are marked as ``unsettled'' (stored as NaN) instead of being forced to a digital 0 or 1. This makes the check robust to slow transitions or ringing: if any output bit has not reached a valid logic level by the sampling time, the issue is surfaced immediately in the processed data rather than silently misclassified.

The result of this first Python stage is a new CSV file with exactly 256 rows (one per input vector) and clean digital values for all bits $X_3$–$X_0$, $Y_3$–$Y_0$, and $Z_7$–$Z_0$.

\subsection{Reconstruction of Integer Operands and Products}

A second Python script interprets the bit fields as integers and checks the arithmetic. A practical concern when exporting from Cadence is that the bit order in the CSV may not match the expected LSB-to-MSB ordering. To guard against this, the script first analyzes the number of transitions on each bit; the bit with the highest toggle count must be the LSB (it has the shortest period). If the fastest-toggling column is labeled $X_3$ instead of $X_0$, for example, the script concludes that the bits are reversed and flips the order in software before proceeding. The same logic is applied to the $Y$ inputs. This automatic detection makes the verification insensitive to column-ordering mistakes in the export step.

With the bit ordering corrected, the integer values are reconstructed as
\begin{align*}
A &= X_0 + 2X_1 + 4X_2 + 8X_3, \\
B &= Y_0 + 2Y_1 + 4Y_2 + 8Y_3, \\
Z &= Z_0 + 2Z_1 + 4Z_2 + 8Z_3 + 16Z_4 + 32Z_5 + 64Z_6 + 128Z_7.
\end{align*}
These become the columns \texttt{A\_val}, \texttt{B\_val}, and \texttt{Z\_val}. The correct reference result is simply
\[
\texttt{Expected} = \texttt{A\_val} \times \texttt{B\_val}.
\]
For each of the 256 vectors the script sets a Boolean flag \texttt{Pass} if \texttt{Z\_val == Expected}. A short summary printed at the end of the run reports how many of the 256 cases passed and, if any failed, lists the mismatched vectors with their input pair, measured product, and expected product. For both the baseline and optimized multipliers, the script reports 256/256 passing cases.

\subsection{Verification Table Visualization}

To make the results easy to interpret at a glance, a final Python script converts the verified dataset into the 16$\times$16 table shown in Fig.~\ref{fig:baseline_verification_table}. The horizontal axis corresponds to the integer value of $X$, the vertical axis corresponds to $Y$, and each cell encodes the product for one $(X,Y)$ pair:

\begin{itemize}
  \item The upper-left number in each cell is the measured product $Z$ from the simulation.
  \item The lower-right number is the ideal product $X \cdot Y$.
  \item The cell background is colored green if these two numbers agree and red otherwise.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{writeup/figures/baseline_verification_visualization.png}
  \caption{Visualization of functional verification for the 4$\times$4 multiplier. For each input pair $(X,Y)$, the upper-right number is the simulated product and the lower-left number is the ideal product $X \cdot Y$. Matching cells are shaded green; any mismatch would appear as a red cell.}
  \label{fig:baseline_verification_table}
\end{figure}

Because the multiplier is correct, every cell in Fig.~\ref{fig:baseline_verification_table} is green and the two numbers in each cell are identical. Any systematic wiring error (for example, swapping two output bits or mis-interpreting signedness) would create a visible pattern of red cells, making this visualization a powerful sanity check in addition to the numerical pass/fail summary. The same scripts are reused for both the baseline and optimized designs, so any changes to the circuit can be re-verified quickly and objectively using identical criteria.

\newpage

% ----------------------------------------------------
\section{Baseline Delay Measurement}

\subsection{Worst–Case Delay Characterization}

To determine the worst–case delay of the $4 \times 4$ array multiplier, we examine the
structure of the adder array and identify the longest carry–dependent computation path.
Each output bit $Z_k$ is formed by a column of full adders (FAs) and half adders (HAs)
whose inputs depend on the partial products $X_i Y_j$ and on the carry values
generated by preceding adders. A delay ``bottleneck'' occurs whenever an adder must
wait for its carry--in to settle before producing either its sum or carry--out. Thus, the
critical path corresponds to the path along which the largest number of carry--dependent
operations occur sequentially.

\subsubsection{Critical Path Identification}

Among all output bits, $Z_7$ is the furthest from the first carry--producing element and
therefore admits the longest possible chain of dependent adders. The first carry is
introduced at the $Z_1$ half adder, and from this point, the carry must propagate
diagonally across the array through a sequence of full adders before eventually reaching
the most significant output $Z_7$. Although multiple geometric paths exist from
$Z_1$ to $Z_7$, each path contains the same total number of carry--in to carry--out
transitions and the same number of carry--in to sum transitions. Because the array
multiplier is topologically symmetric, all such paths have identical logical depth.

For a $4 \times 4$ array, the critical path contains:
\begin{itemize}
    \item six FA/HAs contributing a $\text{Cin} \rightarrow \text{Cout}$ delay (horizontal movement), and
    \item two FA/HAs contributing a $\text{Cin} \rightarrow \text{Sum}$ delay (vertical movement).
\end{itemize}
One representative critical path is:
\[
\text{AND} 
\rightarrow \text{HA}_{Z_1}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}
\rightarrow \text{FA}_{Z_7},
\]
where the ordering reflects successive carry propagation through the array.

\subsubsection{Adder Operating States and Worst--Case Delay Conditions}

An adder (FA or HA) can operate in one of three logical states depending on the values
of its two data inputs $(A, B)$:
\begin{enumerate}
    \item \textbf{Terminate (T):} $(A,B) = (0,0)$.
    In this state, the sum equals $\text{Cin}$, but the carry--out is always $0$.  
    This state prematurely stops the carry chain and therefore does \emph{not} contribute
    to the worst--case delay.
    \item \textbf{Propagate (P):} $(A,B) \in \{(0,1), (1,0)\}$.
    Both sum and carry--out depend on $\text{Cin}$, allowing the incoming carry to ripple
    through the adder.  
    This is the state required to maximize delay.
    \item \textbf{Generate (G):} $(A,B) = (1,1)$.
    The carry--out is forced to logic high regardless of $\text{Cin}$, immediately terminating
    the dependency on previous stages.  
    This state shortens the delay and therefore must be avoided on the critical path.
\end{enumerate}

To excite the worst--case delay, \emph{every adder along the identified critical path must
be placed in the propagate state}. Once all adders are in propagate mode, toggling the
carry--in of the first HA will cause a carry transition to ripple uninterrupted across the
entire chain, from $Z_1$ through to $Z_7$.

\subsubsection{Worst--Case Input Vector Selection}

Let $X_0$ be the least significant bit of $X$ and $Y_0$ the least significant bit of $Y$.
To create a clean rising transition that initiates the carry propagation, $X_0$ is toggled
from $0$ to $1$ while ensuring that the multiplier inputs enforce the propagate state
at all adders along the critical path. One valid worst--case input assignment is:

\[
\begin{array}{c|c|l}
\text{Input Bit} & \text{Value} & \text{Effect} \\ \hline
X_0 & 0 \rightarrow 1 & \text{Introduces a rising carry stimulus at HA}_{Z_1} \\
X_1 & 0 & \text{Ensures propagate state at next HA} \\
X_2 & 0 & \text{Propagate state for deeper FAs} \\
X_3 & 1 & \text{Propagate state in upper FA chain} \\ \hline
Y_0 & 1 & \text{Ensures HA input is capable of seeing Cin transition} \\
Y_1 & 1 & \text{Same as above} \\
Y_2 & 1 & \text{Maintains propagate condition in intermediate FAs} \\
Y_3 & 1 & \text{Maintains propagate condition at upper FAs} \\
\end{array}
\]

With these inputs, all adders along the $Z_1 \rightarrow Z_7$ critical path reside in the
propagate state, ensuring that a single transition at $\text{Cin}$ traverses the full logical
depth of the array. Measuring the time difference between the transition at $X_0$ and
the corresponding transition observed at $Z_7$ yields the worst--case propagation delay
of the baseline multiplier.

\newpage

\subsection{Worst-Case Delay Test Schematic}



\newpage

\subsection{Worst-Case Delay Analysis}

\subsubsection{Worst Case Propagation for Switching Input 0 $\rightarrow$ 1}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup//figures/baseline_delay_low_high_signals.png}
    \caption{Enter Caption}
    \label{fig:baseline_delay_low_high_signals}
\end{figure}

To extract the worst--case delay, the multiplier was excited with the input pattern
described in Section 4.1.3, ensuring that all adders along the
$Z_1 \rightarrow Z_7$ diagonal operate in the propagate state. A rising transition was
applied to $X_0$, which produces the first carry at the $Z_1$ half adder and initiates a
carry ripple through all adders on the critical path.

The propagation delay $t_{\mathrm{PLH}}$ was measured using the $50\%$ voltage points of
the input and output waveforms. The $X_0$ input crosses $0.6\,$V at
\[
t_{\text{in,50\%}} = 10.017844\ \text{ns},
\]
and the most significant output bit $Z_7$ crosses $0.6\,$V at
\[
t_{\text{out,50\%}} = 10.214481\ \text{ns}.
\]

The low--to--high propagation delay is therefore
\[
t_{\mathrm{PLH}}
= t_{\text{out,50\%}} - t_{\text{in,50\%}}
= 10.214481\ \text{ns} - 10.017844\ \text{ns}
= 0.196637\ \text{ns}.
\]

Thus, the worst--case delay of the baseline multiplier is
\[
\boxed{t_{\mathrm{PLH}} = 196.637\ \text{ps}}
\]

\subsubsection{Worst Case Propagation for Switching Input 1 $\rightarrow$ 0}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup//figures/baseline_delay_high_low_signals.png}
    \caption{Enter Caption}
    \label{fig:baseline_delay_high_low_signals}
\end{figure}

The high--to--low worst--case delay was obtained using the same propagate--state
input vector used for the low--to--high measurement. In this case, $X_0$ transitions
from logic high to logic low, and the resulting carry discharge travels along the
$Z_1 \rightarrow Z_7$ critical path.

The $50\%$ crossing of the input $X_0$ waveform occurs at
\[
t_{\text{in,50\%}} = 15.014775\ \text{ns},
\]
while the $50\%$ crossing of the output $Z_7$ waveform occurs at
\[
t_{\text{out,50\%}} = 15.208198\ \text{ns}.
\]

Thus, the high--to--low propagation delay is
\[
t_{\mathrm{PHL}}
= t_{\text{out,50\%}} - t_{\text{in,50\%}}
= 15.208198\ \text{ns} - 15.014775\ \text{ns}
= 0.193423\ \text{ns}.
\]

Therefore, the worst--case $t_{\mathrm{PHL}}$ of the baseline multiplier is
\[
\boxed{t_{\mathrm{PHL}} = 193.423\ \text{ps}}
\]

\newpage

% ----------------------------------------------------
\section{Baseline Active Energy Measurement}

\subsection{Active Energy Test Schematic}



\newpage

\subsection{Active Energy Analysis}

To characterize the dynamic power consumption of the baseline $4\times4$ array multiplier, active switching energy was measured under two representative transitions: (i) a maximum-switching case where every input bit toggles from $0\rightarrow1$, and (ii) an average-switching case where only half of the input bits toggle. In both tests, the multiplier’s VDD rail was isolated on a dedicated supply pin, allowing the instantaneous supply current to be integrated directly using ADE's \texttt{integ()} operator:
\[
E_{\text{active}}=\int_{t_0}^{t_1} i_{\text{VDD}}(t)\,V_{\text{DD}}\,dt.
\]

\subsubsection{Maximum Switching Case}

In the maximum-switching condition, all bits of $X$ and $Y$ transition simultaneously from $0$ to $1$. This forces nearly all internal nodes to switch, producing the highest toggle activity factor and therefore the highest dynamic energy.

Figure~\ref{fig:baseline_active_energy_max_signals} shows the transient switching activity across the multiplier during this input event.  
The ADE current integration result in Fig.~\ref{fig:baseline_active_energy_max_value} reports a total energy consumption of:
\[
E_{\text{max}} = 122.2~\text{fJ}.
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup/figures/baseline_active_energy_max_signals.png}
    \caption{Transient output and internal switching behavior under the maximum-switching input case ($X,Y:0\rightarrow1$).}
    \label{fig:baseline_active_energy_max_signals}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup/figures/baseline_active_energy_max_value.png}
    \caption{Integrated supply current for the maximum-switching case, yielding $E_{\text{max}} = 122.2~\text{fJ}$.}
    \label{fig:baseline_active_energy_max_value}
\end{figure}

\subsubsection{Average Switching Case}

The average-switching case represents a more typical energy profile, where roughly half of the bits in each operand toggle from $0\rightarrow1$. This reduces the number of partial-product transitions and internal adder activations.

The transient behavior for this case is shown in Fig.~\ref{fig:baseline_active_energy_avg_signals}, and the corresponding ADE integration result in Fig.~\ref{fig:baseline_active_energy_avg_value} yields:
\[
E_{\text{avg}} = 89.83~\text{fJ}.
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup/figures/baseline_active_energy_avg_signals.png}
    \caption{Transient output and internal switching behavior under the average-switching input case.}
    \label{fig:baseline_active_energy_avg_signals}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{writeup/figures/baseline_active_energy_avg_value.png}
    \caption{Integrated supply current for the average-switching case, yielding $E_{\text{avg}} = 89.83~\text{fJ}$.}
    \label{fig:baseline_active_energy_avg_value}
\end{figure}

\newpage

% ----------------------------------------------------
\section{Baseline Leakage Energy Measurement}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_FA_leakage_energy_value_case0.png}
    \caption{Enter Caption}
    \label{fig:baseline_FA_leakage_energy_value_case0}
\end{figure}







Min Leakage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_minimum_leakage_value.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

Max Leakage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{writeup//figures/baseline_maximum_leakage_value.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\newpage

\subsection{Leakage Energy Test Schematic}



\newpage

\subsection{Leakage Energy Analysis}

Leakage energy in the 4$\times$4 array multiplier is dominated by the static leakage of the full adders (FAs), which form the majority of the internal logic. Because each FA contributes similarly to the total static energy, the maximum and minimum leakage cases for the entire multiplier can be inferred directly by characterizing a single FA. Once the FA’s highest– and lowest–leakage input states are identified, the multiplier-level input vectors follow naturally by applying the same bit patterns to all FA instances.

\subsubsection{Leakage Energy Model}

The leakage energy over one delay period $T$ is defined as
\[
U_{\text{leak}} = \int_{0}^{T} V_{\mathrm{DD}}\; i(t)\, dt.
\]
Since the leakage current varies only slightly during static input conditions, we approximate it as constant:
\[
U_{\text{leak}} \approx V_{\mathrm{DD}} \cdot i_{\text{leak}} \cdot T.
\]
With $V_{\mathrm{DD}} = 1.2\,$V and a delay period of $T = 196.6\,$ps (the worst-case propagation delay measured for the baseline multiplier), all leakage energies are normalized to this interval.

\subsection{Full-Adder Leakage Characterization}

To characterize FA leakage, we exhaustively sweep all static input combinations:
\[
(\text{In1},\text{In2},C_{\text{in}}) \in \{0,1\}^3.
\]

For each input triplet:
\begin{itemize}
    \item Inputs are tied to constant VDD or GND.
    \item Outputs are loaded with the standard inverter load ($8C_g$).
    \item A 4\,ns transient simulation is run in Spectre.
    \item Leakage energy is extracted using:
\[
U_{\text{leak}} =
\text{integ}\!\big(\text{abs}(i(\text{``/VDD/PLUS''}))\big)\Big|_{t_0}^{t_0+T} \cdot 1.2.
\]
\end{itemize}

A representative FA leakage waveform for the $(0, 0, 0)$ input case is shown in Figure~\ref{fig:baseline_FA_leakage_case_0_signal}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{writeup/figures/baseline_FA_leakage_case_0_signal.png}
    \caption{Example FA leakage current waveform for In1 = In2 = $C_{\text{in}} = 0$.}
    \label{fig:baseline_FA_leakage_case_0_signal}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{c c c | c}
\textbf{In1} & \textbf{In2} & $\mathbf{C_{\text{in}}}$ & \textbf{Leakage Energy (zJ)} \\
\hline
0 & 0 & 0 & 67.27 \\
1 & 0 & 0 & 61.28 \\
0 & 1 & 0 & 62.88 \\
1 & 1 & 0 & 55.29 \\
0 & 0 & 1 & 61.57 \\
1 & 0 & 1 & 57.41 \\
0 & 1 & 1 & 59.83 \\
1 & 1 & 1 & 52.07 \\
\end{tabular}
\caption{Full-adder leakage energy for all static input combinations (integrated over one delay period).}
\label{tab:fa_leak_table}
\end{table}

From Table~\ref{tab:fa_leak_table}, the maximum leakage occurs for
\[
\text{In1} = \text{In2} = C_{\text{in}} = 0,
\]
and the minimum leakage for
\[
\text{In1} = \text{In2} = C_{\text{in}} = 1.
\]
The all-zero case creates several partially-conducting leakage paths through the pull-down devices, while the all-one case produces stronger OFF states, reducing leakage.

\subsection{Multiplier Leakage Measurement}

Using the FA-level results, we apply the corresponding input patterns to the entire multiplier. This ensures every FA experiences the same static state.

\[
\begin{aligned}
&\textbf{Maximum leakage case:} && X_3 X_2 X_1 X_0 = 0000,\quad Y_3 Y_2 Y_1 Y_0 = 0000, \\
&\textbf{Minimum leakage case:} && X_3 X_2 X_1 X_0 = 1111,\quad Y_3 Y_2 Y_1 Y_0 = 1111.
\end{aligned}
\]

The same methodology is used: static inputs, a 4\,ns transient run, and integration over the window $T = 196.6$\,ps on the supply current.

The multiplier-level leakage energies are summarized in Table~\ref{tab:mult_leak_table}.

\begin{table}[H]
\centering
\begin{tabular}{c c | c}
\textbf{X Inputs} & \textbf{Y Inputs} & \textbf{Leakage Energy (zJ)} \\
\hline
0000 & 0000 & 880.9 \\
1111 & 1111 & 775.8 \\
\end{tabular}
\caption{Multiplier leakage energy for maximum and minimum leakage input vectors.}
\label{tab:mult_leak_table}
\end{table}

These results scale consistently with the FA characterization: the multiplier consumes the highest leakage energy when all FAs see the $(0,0,0)$ state, and the lowest leakage when all see $(1,1,1)$. The nearly linear scaling confirms that leakage is dominated by per-FA device leakage rather than interconnect or loading effects.

\newpage

% ----------------------------------------------------
\section{Optimized Design}

\subsection{Worst Case Delay}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\linewidth]{writeup/figures/arraymult_criticalpath.png}
  \caption{Critical-path staircases in a 4×4 Braun array multiplier. X$_0$–X$_3$ are the N bits (multiplicand) and Y$_0$–Y$_3$ are the M bits (multiplier). Figure adapted from materials by Prof. Janakiraman Viraraghavan, IIT Madras.}
\end{figure}

\noindent
We analyze the 4×4 Braun (array) multiplier shown above. The multiplicand bits are X$_0$–X$_3$ (the N bits) and the multiplier bits are Y$_0$–Y$_3$ (the M bits). Each partial product x$_i$y$_j$ is generated by a static-CMOS AND gate, and these partial products are reduced by a regular grid of half adders (HAs) and full adders (FAs). Within a column, carries propagate downward; between columns, sums propagate leftward. The slow paths are the northeast-to-southwest staircases that begin at a right-edge partial product and alternate vertical carry hops with horizontal sum hops until they reach the lower-left outputs.

\paragraph{Delay primitives and convention}
We define three cell delays under the loading and drive conditions that match the tiled array:
\[
t_{\text{and}} \text{ for the partial-product AND,}\quad
t_{\text{carry}} \text{ for an HA/FA carry-out,}\quad
t_{\text{sum}} \text{ for an HA/FA sum-out.}
\]
In static-CMOS adders the sum network is built from XOR/XNOR structures that have larger logical effort and parasitic capacitance than the carry (majority / AND-OR) network, so typically
\[
t_{\text{sum}} > t_{\text{carry}}.
\]

\paragraph{Exciting the limiting staircase}
The worst transition is not the input vector that flips the most bits. It is the transition that (i) toggles a right-edge partial product and (ii) places each encountered adder in propagate mode (a$\oplus$b = 1 and ab = 0), forcing a newly injected carry to ripple through all stages along a staircase to the lower-left.

\paragraph{Counting hops on an M×N array}
For any such staircase in an M×N array, the stage counts are:
\[
\begin{aligned}
\text{carry hops}   &= (M-1) + (N-2),\\
\text{sum hops}     &= (M-1),
\end{aligned}
\]
plus the initial $t_{\text{and}}$ that launches the path. Therefore the generic array-multiplier timing is
\[
t_{\text{array}}(M,N) \;=\; \big[(M-1) + (N-2)\big]\,t_{\text{carry}}
\;+\; (M-1)\,t_{\text{sum}}
\;+\; t_{\text{and}}.
\]

\paragraph{Specialization to the 4×4 case}
With $M=N=4$ we obtain five carry hops and three sum hops:
\[
t_{\text{path}} \;=\; 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}.
\]
In the 4×4 topology the bottom-left FA drives two neighboring outputs: its carry is Z$_7$ and its sum is Z$_6$. The staircase counted above terminates at the sum of this FA, so
\[
\boxed{\,t_{Z_6} = 5\,t_{\text{carry}} + 3\,t_{\text{sum}} + t_{\text{and}}\,}.
\]
Because $t_{\text{sum}} > t_{\text{carry}}$ for static-CMOS FAs, Z$_6$ is slower than the otherwise identical staircase that would end at the carry output Z$_7$. Hence the worst-case delay of the 4×4 array ends at Z$_6$, not Z$_7$.

\paragraph{Implications}
Multiple staircases (such as CP$_1$ and CP$_2$ in the figure) have the same hop counts and are nearly iso-delay, so cell sizing must balance stage effort across the grid. Nevertheless, the decisive last stage is the sum network of the bottom-left FA that produces Z$_6$, and this stage should be weighted accordingly when sizing to minimize the overall worst-case delay.

\subsection{Schematics in Static CMOS}
\subsubsection{Full Adder (FA)}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{writeup/figures/staticcmos_fa_rabaey_annotated.png}
  \caption{Complementary static-CMOS full adder. Figure adapted from Rabaey.}
\end{figure}

\noindent
The full adder computes the carry-out $C_o$ and sum $S$ from inputs $A$, $B$, and $C_i$.  
We employ a complementary static-CMOS realization that shares intermediate logic between
the carry and sum paths. The logic is organized as
\[
C_o = AB + BC_i + AC_i \quad\text{(3-input majority)}, \qquad
S = ABC_i + C_o\,(A + B + C_i).
\]
The schematic implements these expressions with series–parallel $p$- and $n$-networks so that
each output is produced by static pull-up and pull-down paths, guaranteeing full-swing,
monotonic transitions and large noise margins.

\paragraph{Signal flow and stage counts}
The circuit is partitioned around an internal node $X$ that realizes the reorganized carry logic.
\begin{itemize}
  \item \textbf{Path to $C_o$ (two static stages).} A complex CMOS network first forms $X$,
  which captures the majority function. An output inverter then buffers $X$ to produce $C_o$.
  \item \textbf{Path to $S$ (three static stages).} The same $X$ is reused by a second
  static network that combines $X$ with $A$, $B$, and $C_i$ to generate a pre-sum node.
  A final inverter buffers this node to produce $S$.
\end{itemize}
Thus, $C_o$ emerges after two static stages and $S$ after three. While not ``single-stage''
at the pins, these are compact paths with low internal fanout and limited parasitic loading.

\paragraph{Why static CMOS over a bag of gates}
A gate-assembled FA (e.g., library XOR/AND/OR composition) typically yields a deeper chain:
$C_o$ commonly requires two XOR-class stages, and $S$ often traverses a sequence such as
XOR $\rightarrow$ NAND $\rightarrow$ NOT $\rightarrow$ NOR $\rightarrow$ NOT.
Each additional logic boundary adds logical effort and parasitic capacitance, inflating
end-to-end delay and degrading slews. The static-CMOS realization reduces the number of
boundaries, shares $X$ rather than duplicating heavy networks, and eliminates large internal
fanouts between discrete gates. Because both outputs are produced by complementary static
networks and then buffered, signals remain rail-to-rail and monotonic across many cascaded cells.

\paragraph{Fit for the array multiplier}
In the 4$\times$4 Braun array, the slow paths are the northeast-to-southwest staircases of adders,
and the measured worst case terminates at the sum output of the bottom-left FA ($Z_6$).
Replacing multi-gate chains with the compact static implementation shortens the effective
stage depth on these staircases and reduces internal parasitics, lowering the overall delay.

\noindent At the same time, static-CMOS robustness (no ratioing, no dynamic storage) ensures predictable
behavior when many FA cells are tiled: outputs do not droop through the grid, hazards are
suppressed by the static topologies and output inverters, and uniform polarity avoids the need
for multiple FA variants or extra inverters during integration.

\noindent
In summary, the Rabaey complementary static-CMOS FA provides a favorable speed–robustness tradeoff
for the multiplier: fewer and better stages than a bag-of-gates implementation, shared intermediate
logic that limits internal loading, and full-swing stability that holds when cascading many cells.

\subsubsection{Half Adder (HA)}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_xor.png}\hfill
  \includegraphics[width=0.43\linewidth]{writeup/figures/staticcmos_nand.png}
  \caption{Static-CMOS XOR (left) for Sum and NAND (right) used with a following inverter for Carry. Figure adapted from Shakshat Virtual Lab, IIT Guwahati}
\end{figure}

\noindent
The half adder takes one-bit inputs $A$ and $B$ and produces outputs \textit{Sum} and \textit{Carry}. From the truth table,
\[
\text{Sum} = A \oplus B,
\qquad
\text{Carry} = A \cdot B = \overline{\mathrm{NAND}(A,B)}.
\]

\noindent
Implementation in complementary static CMOS is therefore direct:
\begin{itemize}
  \item \textbf{Sum path.} Realize $A \oplus B$ with a static-CMOS XOR gate. This is a single logic stage at the Sum pin with full-swing, monotonic behavior.
  \item \textbf{Carry path.} Realize $A \cdot B$ as a static-CMOS NAND followed by a static inverter: $\text{Carry}=\overline{\mathrm{NAND}(A,B)}$. This is two logic stages at the Carry pin (NAND then inverter), also full swing and monotonic.
\end{itemize}

\noindent
Unlike the full adder, there is no third input to exploit for algebraic sharing or logic reorganization; the minimal Boolean forms are already $A \oplus B$ and $A \cdot B$. Consequently, the “optimized” static-CMOS HA coincides with the baseline bag-of-gates realization: one XOR stage for Sum and a NAND+inverter pair for Carry. This keeps logic depth minimal for each output while retaining the robustness advantages of static CMOS (rail-to-rail levels, large noise margins, and predictable cascading behavior in the multiplier array).

\subsection{Optimizing Gate Sizes Along CP1}

\paragraph{Design variables and device mapping.}
We size five gate types with continuous, positive scale factors
\[
k_{\text{nand}},\;k_{\text{inv}},\;k_{\text{fa1}},\;k_{\text{fa2}},\;k_{\text{xor}}>0.
\]
For the \textbf{NAND} gate, both PMOS and NMOS widths equal $k_{\text{nand}}$.
For the \textbf{INV}, \textbf{FA Stage~1}, \textbf{FA Stage~2}, and \textbf{XOR}, the PMOS width is $2k$ and the NMOS width is $k$, where $k$ is the corresponding scale ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$, $k_{\text{xor}}$).

\paragraph{Assumptions and units.}
We assume: (i) diffusion capacitances are neglected ($C_d=0$);
(ii) all inputs are driven by a minimum-size inverter; (iii) pull-up resistance $R_{\!up}=2R_{\!un}$;
(iv) the final output load is $10C_g$. We measure delay in units of $R_{\!un}C_g$; i.e., we factor out the common multiplier $R_{\!un}C_g$ from every $R_i C_j$ product.

\paragraph{Definition of CP1.}
The critical path instance CP1 crosses the following gate sequence:
\[
\begin{aligned}
&\text{AND }(\text{NAND}+\text{INV});\ 
\text{HA,carry }(\text{NAND}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,carry }(\text{FA1}+\text{INV});\\
&\text{HA,sum }(\text{INV}+\text{XOR});\
\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV});\\
&\text{FA,carry }(\text{FA1}+\text{INV});\
\text{FA,sum }(\text{FA1}+\text{FA2}+\text{INV}).
\end{aligned}
\]

\paragraph{Step-by-step RC model (21 steps).}
Let $(R_i, C_i)$ denote the driving resistance and the load capacitance of step $i$ on CP1, each expressed in the normalized units described above. Using the worst-case edge per step you specified:

\[
\begin{aligned}
R_1&=2,                      & C_1&=2k_{\text{nand}}, \\
R_2&=\frac{2}{k_{\text{nand}}}, & C_2&=3k_{\text{inv}}, \\
R_3&=\frac{2}{k_{\text{inv}}},   & C_3&=2k_{\text{nand}}, \\
R_4&=\frac{2}{k_{\text{nand}}}, & C_4&=3k_{\text{inv}}, \\
R_5&=\frac{2}{k_{\text{inv}}},   & C_5&=6k_{\text{fa1}}, \\
R_6&=\frac{6}{k_{\text{fa1}}},   & C_6&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_7&=\frac{2}{k_{\text{inv}}},   & C_7&=6k_{\text{fa1}}, \\
R_8&=\frac{6}{k_{\text{fa1}}},   & C_8&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_9&=\frac{2}{k_{\text{inv}}},   & C_9&=3k_{\text{inv}}, \\
R_{10}&=\frac{2}{k_{\text{inv}}},& C_{10}&=3k_{\text{xor}}, \\
R_{11}&=\frac{4}{k_{\text{xor}}},& C_{11}&=6k_{\text{fa1}}, \\
R_{12}&=\frac{6}{k_{\text{fa1}}},& C_{12}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{13}&=\frac{2}{k_{\text{inv}}},& C_{13}&=6k_{\text{fa1}}, \\
R_{14}&=\frac{6}{k_{\text{fa1}}},& C_{14}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{15}&=\frac{4}{k_{\text{fa2}}},& C_{15}&=3k_{\text{inv}}, \\
R_{16}&=\frac{2}{k_{\text{inv}}},& C_{16}&=6k_{\text{fa1}}, \\
R_{17}&=\frac{6}{k_{\text{fa1}}},& C_{17}&=3k_{\text{inv}}+3k_{\text{fa1}}, \\
R_{18}&=\frac{2}{k_{\text{inv}}},& C_{18}&=6k_{\text{fa1}}, \\
R_{19}&=\frac{6}{k_{\text{fa1}}},& C_{19}&=3k_{\text{inv}}+3k_{\text{fa2}}, \\
R_{20}&=\frac{4}{k_{\text{fa2}}},& C_{20}&=3k_{\text{inv}}, \\
R_{21}&=\frac{2}{k_{\text{inv}}},& C_{21}&=10. \\
\end{aligned}
\]

\paragraph{Elmore delay objective.}
Let the cumulative resistance up to step $i$ be
\[
S_i \;\triangleq\; \sum_{j=1}^{i} R_j.
\]
The Elmore delay along CP1 is
\[
\tau(k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}})
= \sum_{i=1}^{21} S_i\,C_i.
\]
By construction, $\tau$ is homogeneous in the time unit $R_{\!un}C_g$.

\paragraph{Structure of the derivatives.}
Each $R_i$ is either constant or of the form $\alpha_i/x$ for a single sizing variable $x\in
\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}$; each $C_i$ is either constant or of the form $\beta_i x$ or a sum of such linear terms. Hence
\[
\frac{\partial R_i}{\partial x}=
\begin{cases}
-\dfrac{\alpha_i}{x^2}, & \text{if } R_i=\dfrac{\alpha_i}{x},\\[6pt]
0, & \text{otherwise},
\end{cases}
\qquad
\frac{\partial C_i}{\partial x}=
\begin{cases}
\beta_i, & \text{if } C_i\text{ contains }\beta_i x,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Using $S_i=\sum_{j\le i}R_j$, its derivative satisfies
\[
\frac{\partial S_i}{\partial x}=\sum_{j=1}^{i}\frac{\partial R_j}{\partial x}.
\]
Therefore the gradient components are
\[
\boxed{\quad
\frac{\partial \tau}{\partial x}
= \sum_{i=1}^{21}\left(\frac{\partial S_i}{\partial x}\,C_i
+ S_i\,\frac{\partial C_i}{\partial x}\right)
= \sum_{i=1}^{21}\left(\sum_{j=1}^i \frac{\partial R_j}{\partial x}\right)C_i
+ \sum_{i=1}^{21} S_i\,\frac{\partial C_i}{\partial x}.
\quad}
\]

\paragraph{Coefficient bookkeeping per variable.}
For compactness, we list the steps that contribute to each variable’s partial derivative, with their coefficients $(\alpha,\beta)$:

\begin{itemize}
\item \textbf{$x=k_{\text{nand}}$:}
\[
\begin{aligned}
&\text{Resistance terms: } R_2=\tfrac{2}{k_{\text{nand}}},\ R_4=\tfrac{2}{k_{\text{nand}}}
\ \Rightarrow\ \alpha_{2}=\alpha_{4}=2;\\
&\text{Capacitance terms: } C_1=2k_{\text{nand}},\ C_3=2k_{\text{nand}}
\ \Rightarrow\ \beta_{1}=\beta_{3}=2.
\end{aligned}
\]

\item \textbf{$x=k_{\text{inv}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_3,R_5,R_7,R_9,R_{10},R_{13},R_{16},R_{18},R_{21}
= \tfrac{2}{k_{\text{inv}}}\ \Rightarrow\ \alpha=2\ \text{at those indices};\\
&\text{Capacitance: } C_2,C_4,C_6,C_8,C_9,C_{12},C_{14},C_{15},C_{17},C_{19},C_{20}
= 3k_{\text{inv}}\ \Rightarrow\ \beta=3\ \text{at those indices}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa1}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_6,R_8,R_{12},R_{14},R_{17},R_{19}=\tfrac{6}{k_{\text{fa1}}}
\ \Rightarrow\ \alpha=6\ \text{at those indices};\\
&\text{Capacitance: } C_5,C_7,C_{11},C_{13},C_{16},C_{18}=6k_{\text{fa1}},\\
&\hspace*{25mm}
C_6,C_8,C_{12},C_{14},C_{17},C_{19}\ \text{contain }3k_{\text{fa1}}
\ \Rightarrow\ \beta=6\ \text{or }3\ \text{accordingly}.
\end{aligned}
\]

\item \textbf{$x=k_{\text{fa2}}$:}
\[
\begin{aligned}
&\text{Resistance: } R_{15},R_{20}=\tfrac{4}{k_{\text{fa2}}}
\ \Rightarrow\ \alpha=4\ \text{at }15,20;\\
&\text{Capacitance: } C_{19}=3k_{\text{fa2}}
\ \Rightarrow\ \beta_{19}=3.
\end{aligned}
\]

\item \textbf{$x=k_{\text{xor}}$:}
\[
\text{Resistance: } R_{11}=\tfrac{4}{k_{\text{xor}}}\ (\alpha_{11}=4),\qquad
\text{Capacitance: } C_{10}=3k_{\text{xor}}\ (\beta_{10}=3).
\]
\end{itemize}

\paragraph{Stationarity conditions.}
At the unconstrained interior optimum (all $k>0$), the first-order conditions are
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{inv}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa1}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{fa2}}}=0,\quad
\frac{\partial \tau}{\partial k_{\text{xor}}}=0.
\]
Expanding each with the lists above yields five nonlinear equations:
\[
\boxed{\;
\sum_{i=1}^{21}\Big(\sum_{j=1}^{i} \partial R_j/\partial x\Big)C_i
\;+\;\sum_{i=1}^{21} S_i\,\partial C_i/\partial x \;=\;0,
\qquad x\in\{k_{\text{nand}},k_{\text{inv}},k_{\text{fa1}},k_{\text{fa2}},k_{\text{xor}}\}.
\;}
\]
Concretely, for $x=k_{\text{nand}}$,
\[
\frac{\partial \tau}{\partial k_{\text{nand}}}
=\sum_{i=1}^{21}\!\Bigg(\sum_{j=1}^i\!\left[-\frac{2}{k_{\text{nand}}^2}\,\mathbb{1}_{\{j=2,4\}}\right]\!\Bigg) C_i
\;+\; \sum_{i=1}^{21}\! S_i\,(2\,\mathbb{1}_{\{i=1,3\}})
=0,
\]
and analogous expressions hold for the other variables by substituting their contributing indices and coefficients.

\paragraph{Numerical solution and improvement.}
Solving the five stationarity equations simultaneously for the positive real solution gives the following optimum (continuous sizes, in the normalized units above):
\[
\boxed{
\begin{aligned}
k_{\text{nand}}&\approx 8.27,\quad
k_{\text{inv}}\approx 1.62,\quad
k_{\text{fa1}}\approx 1.79,\\
k_{\text{fa2}}&\approx 1.62,\quad
k_{\text{xor}}\approx 3.15.
\end{aligned}}
\]
With all $k=1$ (baseline), the Elmore delay along CP1 evaluates to
\[
\tau_{\text{base}} \;=\; 4054\ \big[R_{\!un}C_g\big].
\]
At the optimum,
\[
\tau \;=\; 3618.8\ \big[R_{\!un} C_g\big],
\]
which is an \textbf{approx.\ 10.8\%} reduction in the CP1 Elmore constant under the stated assumptions.

\paragraph{Interpretation.}
The optimizer increases $k_{\text{nand}}$ substantially (NANDs appear early and repeatedly), raises $k_{\text{xor}}$ to avoid starving step~11, and keeps the late-stage drivers ($k_{\text{inv}}$, $k_{\text{fa1}}$, $k_{\text{fa2}}$) moderately above unity to balance the large downstream loads, especially the final $10C_g$. The stationarity equations enforce near-equalized per-stage effort over the 21-step chain, preventing the tail (steps 16--21) from dominating while also avoiding excessive upsizing that would inflate upstream capacitive loads.


\newpage

% ----------------------------------------------------
\section{Optimized Functional Verification}
\label{sec:opt_functional_verification}

The optimized 4$\times$4 multiplier was verified using the same
exhaustive, self-checking infrastructure described for the baseline
design in Section~\ref{sec:functional_verification}. The only change in
the testbench is that the device under test (DUT) is now the optimized
schematic; all stimulus generation, sampling times, and post-processing
scripts are reused without modification. This ensures that any
difference in behavior would be attributable to the circuit changes
rather than to the verification environment.

\subsection{Transient Stimulus and Waveforms}

The eight primary inputs $X_0$--$X_3$ and $Y_0$--$Y_3$ are again driven
by a \texttt{vpulse}-based binary counter with periods
$T,2T,4T,8T$ and $16T,32T,64T,128T$, respectively. The transient
simulation runs for $1.28\,\mu\mathrm{s} = 128T$, so that all
$2^8 = 256$ input combinations $(X,Y)$ with $X,Y \in [0,15]$ are
applied once.

The resulting waveforms for the optimized DUT are shown in
Fig.~\ref{fig:opt_signals}. As in the baseline case, the lower traces
are the input bits and the upper traces are the multiplier outputs
$Z_0$--$Z_7$, which respond deterministically to each new input vector.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/optimized_verification_signals.png}
  \caption{Transient response of the optimized 4$\times$4 multiplier under the same exhaustive binary-count stimulus used for the baseline design.}
  \label{fig:opt_signals}
\end{figure}

\subsection{Digital Reconstruction and Result Table}

The simulator output is exported to CSV and processed by the same pair
of Python scripts used for the baseline:

\begin{itemize}
  \item The first script samples each signal at times
        $t_k = kT/2 + T/4$ (midpoint of every stable interval), converts
        voltages above $1.0\,\mathrm{V}$ to logic ``1'', voltages below
        $0.2\,\mathrm{V}$ to logic ``0'', and flags any intermediate
        values as unsettled.
  \item The second script automatically detects bit ordering from the
        toggle rates, reconstructs the integer operands
        $A$ and $B$, forms the simulated product $Z$, and compares it to
        the ideal product $A \cdot B$ for all $256$ input vectors.
\end{itemize}

For the optimized multiplier the script again reports 256/256 passing
cases; no unsettled outputs are observed at the sampling instants. The
results are visualized in the 16$\times$16 table of
Fig.~\ref{fig:opt_verification_table}, constructed in the same style as
Fig.~\ref{fig:baseline_verification_table}. Each cell corresponds to one
input pair $(X,Y)$, with the simulated product shown in the upper-left
of the cell and the ideal product $X \cdot Y$ in the lower-right; cells
are shaded green when the two numbers agree.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{writeup/figures/optimized_verification_visualization.png}
  \caption{Verification table for the optimized 4$\times$4 multiplier. For each input pair $(X,Y)$, the upper-left number is the simulated product and the lower-right number is the ideal product $X \cdot Y$. All 256 cases match, so every cell is green.}
  \label{fig:opt_verification_table}
\end{figure}

Because the optimized implementation passes exactly the same exhaustive
test as the baseline, with identical stimulus and checking criteria, we
can conclude that the optimization preserves the functional behavior of
the multiplier while enabling the delay and energy improvements
discussed in later sections.

\newpage

% ----------------------------------------------------
\section{Optimized Delay Measurement}

Delay Measurement Simulation

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{writeup//figures/optimized_delay_ADEL.png}
    \caption{Enter Caption}
    \label{fig:optimized_delay_ADEL}
\end{figure}

\newpage

% ----------------------------------------------------
\section{Optimized Active Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Optimized Leakage Energy Measurement}



\newpage

% ----------------------------------------------------
\section{Summary of Optimization Process}



\newpage

% ----------------------------------------------------
% ----------------------------------------------------
\section{Design Exploration}
\label{sec:design_exploration}

Although the final implementation uses only complementary static-CMOS gates, we
did explore a range of alternative logic styles and adder structures before
settling on the architecture presented in this report. Our goal was to
demonstrate meaningful design choices \emph{inside} the 4$\times$4 multiplier
project (already more advanced than the alternative 8-bit adder option), without
sacrificing robustness in a tiled, transistor-level implementation.

\subsection{Multiplier Architectures Considered}

At the architectural level we considered three broad multiplier families:

\begin{itemize}
  \item \textbf{Serial (shift-and-add) multipliers}, which reuse a single adder
        across multiple cycles. These are attractive for low-area, low-power
        designs but introduce a multi-cycle control path and are less aligned
        with the ``bit-sliced datapath'' emphasis of the project.
  \item \textbf{Tree-based multipliers} (e.g., Wallace or Dadda trees), which
        use carry-save reduction to shorten the critical path. For 4-bit
        operands, however, the overhead of extra compressor cells and routing is
        comparable to the nominal gain, and the resulting layout is less
        regular.
  \item \textbf{Braun (array) multipliers}, which generate all partial products
        with AND gates and reduce them with a regular grid of HAs and FAs.
        This style has a very regular floorplan and exposes clear critical paths
        (CP$_1$, CP$_2$) that are easy to analyze and optimize.
\end{itemize}

Given the small operand size and the project’s focus on schematic-level cell
design and sizing, we chose the Braun array. It provides enough structure to
make ``critical-path aware'' optimization meaningful, while remaining simple
enough to verify exhaustively at the transistor level.

\subsection{Adder Cell Topologies}

Within the chosen array structure we then considered several full-adder and
half-adder realizations:

\begin{itemize}
  \item A \textbf{pure gate-assembled FA/HA}, built from library-style XOR,
        NAND, NOR, and inverter cells (our baseline). This option is conceptually
        straightforward but produces relatively deep logic chains on the
        critical staircases.
  \item A \textbf{mirror-style static-CMOS FA} (Rabaey-style), in which the
        pull-up and pull-down networks implement the carry logic directly and
        share internal nodes between Sum and Carry. This reduces the number of
        distinct logic stages and internal fanouts.
  \item \textbf{Pass-transistor and transmission-gate adders}, which can reduce
        transistor count and input capacitance at the cost of more complex
        signal-level behavior (level degradation, charge sharing, etc.).
\end{itemize}

We briefly prototyped pass-transistor based XOR and FA variants at the schematic
level, but ultimately focused our optimization effort on the complementary
static-CMOS FA and HA. This choice aligned better with the goal of a robust
tiled array: the static-CMOS cells provide full-swing, monotonic outputs and
simple noise-margin reasoning when cascaded many times.

\subsection{Logic Styles: Static CMOS vs. Ratioed / Pass-Transistor Logic}

During the exploration phase we also considered more aggressive logic styles,
specifically ratioed logic (e.g., pseudo-NMOS) and pass-transistor/transmission
gate networks, as potential ways to speed up the critical path or reduce area.

\paragraph{Ratioed logic.}

Ratioed logic can implement certain functions with fewer transistors than
complementary static CMOS, potentially reducing load capacitance. However, it
comes with several drawbacks that are particularly problematic in this project:

\begin{itemize}
  \item \textbf{Static power consumption}: a permanently on pull-up device
        leads to DC current whenever the pull-down network conducts, which is
        undesirable in a deeply cascaded structure.
  \item \textbf{Reduced noise margins and level restoration requirements}:
        the logic levels depend on device strength ratios; variations in process,
        voltage, or temperature can move outputs away from solid rail-to-rail
        values. Over multiple stages this can accumulate into functional
        uncertainty.
  \item \textbf{Cascading into static-CMOS array cells}: mixing ratioed stages
        with static-CMOS adders along long paths (e.g., CP$_1$ and CP$_2$) makes
        it harder to guarantee that all internal nodes stay in safe regions
        under worst-case switching.
\end{itemize}

Given that our multiplier already tiles many bit-slices and that we verify at
the transistor level rather than with idealized models, we decided not to push
the design further into ratioed logic. The potential delay savings did not
justify the added verification burden and reliability risk in this use case.

\paragraph{Pass-transistor and transmission-gate logic.}

Pass-transistor and transmission-gate structures can be highly efficient for
XOR/XNOR and multiplexing, and many published ``fast adders'' use them. We did
look at such adders, but several issues emerged when mapping them into our
array:

\begin{itemize}
  \item \textbf{Level degradation in NMOS-only passes}: unless explicitly
        followed by restoration inverters, high levels passed through NMOS-only
        networks can degrade by a threshold voltage. This complicates noise
        margin analysis when these nodes drive further CMOS stages.
  \item \textbf{Charge sharing and dynamic behavior}: internal nodes in
        pass-transistor networks can float or experience charge sharing between
        configurations, which is harder to reason about across many cascaded
        cells and worst-case input patterns.
  \item \textbf{Control signal routing and layout complexity}: in a small,
        regular 4$\times$4 array, the control routing overhead for
        transmission-gate structures can offset their transistor-count
        advantage.
\end{itemize}

We concluded that, for this particular project, the verification and robustness
costs of a heavily pass-transistor-based array outweighed the benefits. Since
we were already taking on the more advanced ``4$\times$4 transistor-level
multiplier'' project (rather than the simpler 8-bit adder option), we chose to
focus our design effort on a well-understood, fully static-CMOS style and to
explore optimization via \emph{cell topology} and \emph{transistor sizing}
instead.

\subsection{Final Design Focus}

After this exploration, our final design strategy was:

\begin{itemize}
  \item Keep the \textbf{architecture} simple and regular: a 4$\times$4 Braun
        array with static-CMOS partial-product ANDs, HAs, and FAs.
  \item Use the \textbf{baseline gate-assembled FA/HA} as a functional
        reference, then adopt a \textbf{complementary static-CMOS FA} with
        shared internal nodes as the optimized cell.
  \item Perform \textbf{systematic sizing optimization} along the identified
        critical path (CP$_1$), using an Elmore-delay model and continuous size
        variables to reduce the worst-case delay while keeping the logic style
        robust and fully static.
\end{itemize}

In other words, we did explore more exotic logic families and alternative
architectures but ultimately chose to keep the logic style conservative and
focus our ``advanced'' work on transistor-level optimization and timing
analysis within the static-CMOS 4$\times$4 array framework.




\newpage

% ----------------------------------------------------
\section{Conclusion}



\end{document}
